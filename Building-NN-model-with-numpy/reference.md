# æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ä»»åŠ¡

æœ¬èŠ‚å°†ä»¥â€œæ³¢å£«é¡¿æˆ¿ä»·â€ä»»åŠ¡ä¸ºä¾‹ï¼Œå‘è¯»è€…ä»‹ç»ä½¿ç”¨Pythonè¯­è¨€å’ŒNumpyåº“æ¥æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹å’Œæ“ä½œæ–¹æ³•ã€‚

æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æ˜¯ä¸€ä¸ªç»å…¸çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œç±»ä¼¼äºç¨‹åºå‘˜ä¸–ç•Œçš„â€œHello Worldâ€ã€‚å’Œå¤§å®¶å¯¹æˆ¿ä»·çš„æ™®éè®¤çŸ¥ç›¸åŒï¼Œæ³¢å£«é¡¿åœ°åŒºçš„æˆ¿ä»·æ˜¯ç”±è¯¸å¤šå› ç´ å½±å“çš„ã€‚è¯¥æ•°æ®é›†ç»Ÿè®¡äº†13ç§å¯èƒ½å½±å“æˆ¿ä»·çš„å› ç´ å’Œè¯¥ç±»å‹æˆ¿å±‹çš„å‡ä»·ï¼ŒæœŸæœ›æ„å»ºä¸€ä¸ªåŸºäº13ä¸ªå› ç´ è¿›è¡Œæˆ¿ä»·é¢„æµ‹çš„æ¨¡å‹ï¼Œå¦‚ **å›¾1** æ‰€ç¤ºã€‚
<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/52eba82f42d34f6480fe2ab01db3b594f17df96ea7d84802bc524649cb4213a0" width="500" hegiht="" ></center>
<center><br>å›¾1ï¼šæ³¢å£«é¡¿æˆ¿ä»·å½±å“å› ç´ ç¤ºæ„å›¾</br></center>
<br></br>

å¯¹äºé¢„æµ‹é—®é¢˜ï¼Œå¯ä»¥æ ¹æ®é¢„æµ‹è¾“å‡ºçš„ç±»å‹æ˜¯è¿ç»­çš„å®æ•°å€¼ï¼Œè¿˜æ˜¯ç¦»æ•£çš„æ ‡ç­¾ï¼ŒåŒºåˆ†ä¸ºå›å½’ä»»åŠ¡å’Œåˆ†ç±»ä»»åŠ¡ã€‚å› ä¸ºæˆ¿ä»·æ˜¯ä¸€ä¸ªè¿ç»­å€¼ï¼Œæ‰€ä»¥æˆ¿ä»·é¢„æµ‹æ˜¾ç„¶æ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ã€‚ä¸‹é¢æˆ‘ä»¬å°è¯•ç”¨æœ€ç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¹¶ç”¨ç¥ç»ç½‘ç»œæ¥å®ç°è¿™ä¸ªæ¨¡å‹ã€‚

## çº¿æ€§å›å½’æ¨¡å‹

å‡è®¾æˆ¿ä»·å’Œå„å½±å“å› ç´ ä¹‹é—´èƒ½å¤Ÿç”¨çº¿æ€§å…³ç³»æ¥æè¿°ï¼š

$$
y = {\sum_{j=1}^Mx_j w_j} + b
$$


æ¨¡å‹çš„æ±‚è§£å³æ˜¯é€šè¿‡æ•°æ®æ‹Ÿåˆå‡ºæ¯ä¸ª$w_j$å’Œ$b$ã€‚å…¶ä¸­ï¼Œ$w_j$å’Œ$b$åˆ†åˆ«è¡¨ç¤ºè¯¥çº¿æ€§æ¨¡å‹çš„æƒé‡å’Œåç½®ã€‚ä¸€ç»´æƒ…å†µä¸‹ï¼Œ$w_j$ å’Œ $b$ æ˜¯ç›´çº¿çš„æ–œç‡å’Œæˆªè·ã€‚

çº¿æ€§å›å½’æ¨¡å‹ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼ˆLossï¼‰ï¼Œç”¨ä»¥è¡¡é‡é¢„æµ‹æˆ¿ä»·å’ŒçœŸå®æˆ¿ä»·çš„å·®å¼‚ï¼Œå…¬å¼å¦‚ä¸‹ï¼š

$$
MSE = \frac{1}{n} \sum_{i=1}^n(\hat{Y_i} - {Y_i})^{2}
$$


------
**æ€è€ƒï¼š**

ä¸ºä»€ä¹ˆè¦ä»¥å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°ï¼Ÿå³å°†æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒæ ·æœ¬ä¸Šçš„é¢„æµ‹è¯¯å·®åŠ å’Œï¼Œæ¥è¡¡é‡æ•´ä½“æ ·æœ¬çš„å‡†ç¡®æ€§ã€‚è¿™æ˜¯å› ä¸ºæŸå¤±å‡½æ•°çš„è®¾è®¡ä¸ä»…ä»…è¦è€ƒè™‘â€œåˆç†æ€§â€ï¼ŒåŒæ ·éœ€è¦è€ƒè™‘â€œæ˜“è§£æ€§â€ï¼Œè¿™ä¸ªé—®é¢˜åœ¨åé¢çš„å†…å®¹ä¸­ä¼šè¯¦ç»†é˜è¿°ã€‚

------

## çº¿æ€§å›å½’æ¨¡å‹çš„ç¥ç»ç½‘ç»œç»“æ„

ç¥ç»ç½‘ç»œçš„æ ‡å‡†ç»“æ„ä¸­æ¯ä¸ªç¥ç»å…ƒç”±åŠ æƒå’Œä¸éçº¿æ€§å˜æ¢æ„æˆï¼Œç„¶åå°†å¤šä¸ªç¥ç»å…ƒåˆ†å±‚çš„æ‘†æ”¾å¹¶è¿æ¥å½¢æˆç¥ç»ç½‘ç»œã€‚çº¿æ€§å›å½’æ¨¡å‹å¯ä»¥è®¤ä¸ºæ˜¯ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€ç§æç®€ç‰¹ä¾‹ï¼Œæ˜¯ä¸€ä¸ªåªæœ‰åŠ æƒå’Œã€æ²¡æœ‰éçº¿æ€§å˜æ¢çš„ç¥ç»å…ƒï¼ˆæ— éœ€å½¢æˆç½‘ç»œï¼‰ï¼Œå¦‚ **å›¾2** æ‰€ç¤ºã€‚
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/ac1f84e0c10e4979bebad9ebc1f4a870c76a4f5c0af543beb21a607efebfb576" width="300" hegiht="" ></center>
<center><br>å›¾2ï¼šçº¿æ€§å›å½’æ¨¡å‹çš„ç¥ç»ç½‘ç»œç»“æ„</br></center>
<br></br>

# æ„å»ºæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ä»»åŠ¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹

æ·±åº¦å­¦ä¹ ä¸ä»…å®ç°äº†å®ç°æ¨¡å‹çš„ç«¯åˆ°ç«¯å­¦ä¹ ï¼Œè¿˜æ¨åŠ¨äº†äººå·¥æ™ºèƒ½è¿›å…¥å·¥ä¸šå¤§ç”Ÿäº§é˜¶æ®µï¼Œäº§ç”Ÿäº†æ ‡å‡†åŒ–ã€è‡ªåŠ¨åŒ–å’Œæ¨¡å—åŒ–çš„é€šç”¨æ¡†æ¶ã€‚ä¸åŒåœºæ™¯çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å…·å…·å¤‡ä¸€å®šçš„é€šç”¨æ€§ï¼Œäº”ä¸ªæ­¥éª¤å³å¯å®Œæˆæ¨¡å‹çš„æ„å»ºå’Œè®­ç»ƒï¼Œå¦‚ **å›¾3** æ‰€ç¤ºã€‚
<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/1ffe7bba8205457db1f3df637686c961403eaa84bfec4ebf822cefdea2e4a001" width="800" hegiht="" ></center>
<center><br>å›¾3ï¼šæ„å»ºç¥ç»ç½‘ç»œ/æ·±åº¦å­¦ä¹ æ¨¡å‹çš„åŸºæœ¬æ­¥éª¤</br></center>
<br></br>

æ­£æ˜¯ç”±äºæ·±åº¦å­¦ä¹ çš„å»ºæ¨¡å’Œè®­ç»ƒçš„è¿‡ç¨‹å­˜åœ¨é€šç”¨æ€§ï¼Œåœ¨æ„å»ºä¸åŒçš„æ¨¡å‹æ—¶ï¼Œåªæœ‰æ¨¡å‹ä¸‰è¦ç´ ä¸åŒï¼Œå…¶å®ƒæ­¥éª¤åŸºæœ¬ä¸€è‡´ï¼Œæ·±åº¦å­¦ä¹ æ¡†æ¶æ‰æœ‰ç”¨æ­¦ä¹‹åœ°ã€‚

## æ•°æ®å¤„ç†

æ•°æ®å¤„ç†åŒ…å«äº”ä¸ªéƒ¨åˆ†ï¼šæ•°æ®å¯¼å…¥ã€æ•°æ®å½¢çŠ¶å˜æ¢ã€æ•°æ®é›†åˆ’åˆ†ã€æ•°æ®å½’ä¸€åŒ–å¤„ç†å’Œå°è£…load dataå‡½æ•°ã€‚æ•°æ®é¢„å¤„ç†åï¼Œæ‰èƒ½è¢«æ¨¡å‹è°ƒç”¨ã€‚

------
**è¯´æ˜ï¼š**

* æœ¬æ•™ç¨‹ä¸­çš„ä»£ç éƒ½å¯ä»¥åœ¨AIStudioä¸Šç›´æ¥è¿è¡Œï¼ŒPrintç»“æœéƒ½æ˜¯åŸºäºç¨‹åºçœŸå®è¿è¡Œçš„ç»“æœã€‚
* ç”±äºæ˜¯çœŸå®æ¡ˆä¾‹ï¼Œä»£ç ä¹‹å‰å­˜åœ¨ä¾èµ–å…³ç³»ï¼Œå› æ­¤éœ€è¦è¯»è€…é€æ¡ã€å…¨éƒ¨è¿è¡Œï¼Œå¦åˆ™ä¼šå¯¼è‡´Printæ—¶æŠ¥é”™ã€‚

------

### è¯»å…¥æ•°æ®

é€šè¿‡å¦‚ä¸‹ä»£ç è¯»å…¥æ•°æ®ï¼Œäº†è§£ä¸‹æ³¢å£«é¡¿æˆ¿ä»·çš„æ•°æ®é›†ç»“æ„ï¼Œæ•°æ®å­˜æ”¾åœ¨æœ¬åœ°ç›®å½•ä¸‹housing.dataæ–‡ä»¶ä¸­ã€‚


```python
# å¯¼å…¥éœ€è¦ç”¨åˆ°çš„package
import numpy as np
import json
# è¯»å…¥è®­ç»ƒæ•°æ®
datafile = './work/housing.data'
data = np.fromfile(datafile, sep=' ')
data
```




    array([6.320e-03, 1.800e+01, 2.310e+00, ..., 3.969e+02, 7.880e+00,
           1.190e+01])



### æ•°æ®å½¢çŠ¶å˜æ¢

ç”±äºè¯»å…¥çš„åŸå§‹æ•°æ®æ˜¯1ç»´çš„ï¼Œæ‰€æœ‰æ•°æ®éƒ½è¿åœ¨ä¸€èµ·ã€‚å› æ­¤éœ€è¦æˆ‘ä»¬å°†æ•°æ®çš„å½¢çŠ¶è¿›è¡Œå˜æ¢ï¼Œå½¢æˆä¸€ä¸ª2ç»´çš„çŸ©é˜µï¼Œæ¯è¡Œä¸ºä¸€ä¸ªæ•°æ®æ ·æœ¬ï¼ˆ14ä¸ªå€¼ï¼‰ï¼Œæ¯ä¸ªæ•°æ®æ ·æœ¬åŒ…å«13ä¸ªXï¼ˆå½±å“æˆ¿ä»·çš„ç‰¹å¾ï¼‰å’Œä¸€ä¸ªYï¼ˆè¯¥ç±»å‹æˆ¿å±‹çš„å‡ä»·ï¼‰ã€‚


```python
# è¯»å…¥ä¹‹åçš„æ•°æ®è¢«è½¬åŒ–æˆ1ç»´arrayï¼Œå…¶ä¸­arrayçš„ç¬¬0-13é¡¹æ˜¯ç¬¬ä¸€æ¡æ•°æ®ï¼Œç¬¬14-27é¡¹æ˜¯ç¬¬äºŒæ¡æ•°æ®ï¼Œä»¥æ­¤ç±»æ¨.... 
# è¿™é‡Œå¯¹åŸå§‹æ•°æ®åšreshapeï¼Œå˜æˆN x 14çš„å½¢å¼
feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE','DIS', 
                 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
feature_num = len(feature_names)
data = data.reshape([data.shape[0] // feature_num, feature_num])
```


```python
# æŸ¥çœ‹æ•°æ®
x = data[0]
print(x.shape)
print(x)
```

    ()
    0.00632


### æ•°æ®é›†åˆ’åˆ†

å°†æ•°æ®é›†åˆ’åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­è®­ç»ƒé›†ç”¨äºç¡®å®šæ¨¡å‹çš„å‚æ•°ï¼Œæµ‹è¯•é›†ç”¨äºè¯„åˆ¤æ¨¡å‹çš„æ•ˆæœã€‚ä¸ºä»€ä¹ˆè¦å¯¹æ•°æ®é›†è¿›è¡Œæ‹†åˆ†ï¼Œè€Œä¸èƒ½ç›´æ¥åº”ç”¨äºæ¨¡å‹è®­ç»ƒå‘¢ï¼Ÿè¿™ä¸å­¦ç”Ÿæ—¶ä»£çš„æˆè¯¾å’Œè€ƒè¯•å…³ç³»æ¯”è¾ƒç±»ä¼¼ï¼Œå¦‚ **å›¾4** æ‰€ç¤ºã€‚

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/a2c8c8cef82846678e3050b29d253aa6d1101eba0ceb4b099f42e3ad33d96928" width="600" hegiht="" ></center>
<center><br>å›¾4ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ‹†åˆ†çš„æ„ä¹‰</br></center>
<br></br>

ä¸Šå­¦æ—¶æ€»æœ‰ä¸€äº›è‡ªä½œèªæ˜çš„åŒå­¦ï¼Œå¹³æ—¶ä¸è®¤çœŸå­¦ä¹ ï¼Œè€ƒè¯•å‰ä¸´é˜µæŠ±ä½›è„šï¼Œå°†ä¹ é¢˜æ­»è®°ç¡¬èƒŒä¸‹æ¥ï¼Œä½†æ˜¯æˆç»©å¾€å¾€å¹¶ä¸å¥½ã€‚å› ä¸ºå­¦æ ¡æœŸæœ›å­¦ç”ŸæŒæ¡çš„æ˜¯çŸ¥è¯†ï¼Œè€Œä¸ä»…ä»…æ˜¯ä¹ é¢˜æœ¬èº«ã€‚å¦å‡ºæ–°çš„è€ƒé¢˜ï¼Œæ‰èƒ½é¼“åŠ±å­¦ç”ŸåŠªåŠ›å»æŒæ¡ä¹ é¢˜èƒŒåçš„åŸç†ã€‚åŒæ ·æˆ‘ä»¬æœŸæœ›æ¨¡å‹å­¦ä¹ çš„æ˜¯ä»»åŠ¡çš„æœ¬è´¨è§„å¾‹ï¼Œè€Œä¸æ˜¯è®­ç»ƒæ•°æ®æœ¬èº«ï¼Œæ¨¡å‹è®­ç»ƒæœªä½¿ç”¨çš„æ•°æ®ï¼Œæ‰èƒ½æ›´çœŸå®çš„è¯„ä¼°æ¨¡å‹çš„æ•ˆæœã€‚

åœ¨æœ¬æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†80%çš„æ•°æ®ç”¨ä½œè®­ç»ƒé›†ï¼Œ20%ç”¨ä½œæµ‹è¯•é›†ï¼Œå®ç°ä»£ç å¦‚ä¸‹ã€‚é€šè¿‡æ‰“å°è®­ç»ƒé›†çš„å½¢çŠ¶ï¼Œå¯ä»¥å‘ç°å…±æœ‰404ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬å«æœ‰13ä¸ªç‰¹å¾å’Œ1ä¸ªé¢„æµ‹å€¼ã€‚


```python
ratio = 0.8
offset = int(data.shape[0] * ratio)
training_data = data[:offset]
training_data.shape
```




    (404, 14)



### æ•°æ®å½’ä¸€åŒ–å¤„ç†

å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æ¯ä¸ªç‰¹å¾çš„å–å€¼ç¼©æ”¾åˆ°0~1ä¹‹é—´ã€‚è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ï¼šä¸€æ˜¯æ¨¡å‹è®­ç»ƒæ›´é«˜æ•ˆï¼›äºŒæ˜¯ç‰¹å¾å‰çš„æƒé‡å¤§å°å¯ä»¥ä»£è¡¨è¯¥å˜é‡å¯¹é¢„æµ‹ç»“æœçš„è´¡çŒ®åº¦ï¼ˆå› ä¸ºæ¯ä¸ªç‰¹å¾å€¼æœ¬èº«çš„èŒƒå›´ç›¸åŒï¼‰ã€‚


```python
# è®¡ç®—trainæ•°æ®é›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼
maximums, minimums, avgs = \
                     training_data.max(axis=0), \
                     training_data.min(axis=0), \
     training_data.sum(axis=0) / training_data.shape[0]
# å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
for i in range(feature_num):
    #print(maximums[i], minimums[i], avgs[i])
    data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])
```

### å°è£…æˆload dataå‡½æ•°

å°†ä¸Šè¿°å‡ ä¸ªæ•°æ®å¤„ç†æ“ä½œå°è£…æˆload dataå‡½æ•°ï¼Œä»¥ä¾¿ä¸‹ä¸€æ­¥æ¨¡å‹çš„è°ƒç”¨ï¼Œä»£ç å¦‚ä¸‹ã€‚


```python
def load_data():
    # ä»æ–‡ä»¶å¯¼å…¥æ•°æ®
    datafile = './work/housing.data'
    data = np.fromfile(datafile, sep=' ')

    # æ¯æ¡æ•°æ®åŒ…æ‹¬14é¡¹ï¼Œå…¶ä¸­å‰é¢13é¡¹æ˜¯å½±å“å› ç´ ï¼Œç¬¬14é¡¹æ˜¯ç›¸åº”çš„æˆ¿å±‹ä»·æ ¼ä¸­ä½æ•°
    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \
                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]
    feature_num = len(feature_names)

    # å°†åŸå§‹æ•°æ®è¿›è¡ŒReshapeï¼Œå˜æˆ[N, 14]è¿™æ ·çš„å½¢çŠ¶
    data = data.reshape([data.shape[0] // feature_num, feature_num])

    # å°†åŸæ•°æ®é›†æ‹†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # è¿™é‡Œä½¿ç”¨80%çš„æ•°æ®åšè®­ç»ƒï¼Œ20%çš„æ•°æ®åšæµ‹è¯•
    # æµ‹è¯•é›†å’Œè®­ç»ƒé›†å¿…é¡»æ˜¯æ²¡æœ‰äº¤é›†çš„
    ratio = 0.8
    offset = int(data.shape[0] * ratio)
    training_data = data[:offset]

    # è®¡ç®—trainæ•°æ®é›†çš„æœ€å¤§å€¼ï¼Œæœ€å°å€¼ï¼Œå¹³å‡å€¼
    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \
                                 training_data.sum(axis=0) / training_data.shape[0]

    # å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    for i in range(feature_num):
        #print(maximums[i], minimums[i], avgs[i])
        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])

    # è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ’åˆ†æ¯”ä¾‹
    training_data = data[:offset]
    test_data = data[offset:]
    return training_data, test_data
```


```python
# è·å–æ•°æ®
training_data, test_data = load_data()
x = training_data[:, :-1]
y = training_data[:, -1:]
```


```python
# æŸ¥çœ‹æ•°æ®
print(x[0])
print(y[0])
```

    [-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817
      0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112
     -0.17590923]
    [-0.00390539]


## æ¨¡å‹è®¾è®¡

æ¨¡å‹è®¾è®¡æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹å…³é”®è¦ç´ ä¹‹ä¸€ï¼Œä¹Ÿç§°ä¸ºç½‘ç»œç»“æ„è®¾è®¡ï¼Œç›¸å½“äºæ¨¡å‹çš„å‡è®¾ç©ºé—´ï¼Œå³å®ç°æ¨¡å‹â€œå‰å‘è®¡ç®—â€ï¼ˆä»è¾“å…¥åˆ°è¾“å‡ºï¼‰çš„è¿‡ç¨‹ã€‚

å¦‚æœå°†è¾“å…¥ç‰¹å¾å’Œè¾“å‡ºé¢„æµ‹å€¼å‡ä»¥å‘é‡è¡¨ç¤ºï¼Œè¾“å…¥ç‰¹å¾$x$æœ‰13ä¸ªåˆ†é‡ï¼Œ$y$æœ‰1ä¸ªåˆ†é‡ï¼Œé‚£ä¹ˆå‚æ•°æƒé‡çš„å½¢çŠ¶ï¼ˆshapeï¼‰æ˜¯
$$
13\times1
$$
ã€‚å‡è®¾æˆ‘ä»¬ä»¥å¦‚ä¸‹ä»»æ„æ•°å­—èµ‹å€¼å‚æ•°åšåˆå§‹åŒ–ï¼š
$$
w=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]
$$



```python
w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, -0.1, -0.2, -0.3, -0.4, 0.0]
w = np.array(w).reshape([13, 1])
```

å–å‡ºç¬¬1æ¡æ ·æœ¬æ•°æ®ï¼Œè§‚å¯Ÿæ ·æœ¬çš„ç‰¹å¾å‘é‡ä¸å‚æ•°å‘é‡ç›¸ä¹˜çš„ç»“æœã€‚


```python
x1=x[0]
t = np.dot(x1, w)
print(t)
```

    [0.03395597]

å®Œæ•´çš„çº¿æ€§å›å½’å…¬å¼ï¼Œè¿˜éœ€è¦åˆå§‹åŒ–åç§»é‡$b$ï¼ŒåŒæ ·éšæ„èµ‹åˆå€¼-0.2ã€‚é‚£ä¹ˆï¼Œçº¿æ€§å›å½’æ¨¡å‹çš„å®Œæ•´è¾“å‡ºæ˜¯
$$
z=t+b
$$

ï¼Œè¿™ä¸ªä»ç‰¹å¾å’Œå‚æ•°è®¡ç®—è¾“å‡ºå€¼çš„è¿‡ç¨‹ç§°ä¸ºâ€œå‰å‘è®¡ç®—â€ã€‚


```python
b = -0.2
z = t + b
print(z)
```

    [-0.16604403]


å°†ä¸Šè¿°è®¡ç®—é¢„æµ‹è¾“å‡ºçš„è¿‡ç¨‹ä»¥â€œç±»å’Œå¯¹è±¡â€çš„æ–¹å¼æ¥æè¿°ï¼Œç±»æˆå‘˜å˜é‡æœ‰å‚æ•°$w$å’Œ$b$ã€‚é€šè¿‡å†™ä¸€ä¸ªforwardå‡½æ•°ï¼ˆä»£è¡¨â€œå‰å‘è®¡ç®—â€ï¼‰å®Œæˆä¸Šè¿°ä»ç‰¹å¾å’Œå‚æ•°åˆ°è¾“å‡ºé¢„æµ‹å€¼çš„è®¡ç®—è¿‡ç¨‹ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚


```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œ
        # æ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
```

åŸºäºNetworkç±»çš„å®šä¹‰ï¼Œæ¨¡å‹çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºã€‚


```python
net = Network(13)
x1 = x[0]
y1 = y[0]
z = net.forward(x1)
print(z)
```

    [-0.63182506]


## è®­ç»ƒé…ç½®

æ¨¡å‹è®¾è®¡å®Œæˆåï¼Œéœ€è¦é€šè¿‡è®­ç»ƒé…ç½®å¯»æ‰¾æ¨¡å‹çš„æœ€ä¼˜å€¼ï¼Œå³é€šè¿‡æŸå¤±å‡½æ•°æ¥è¡¡é‡æ¨¡å‹çš„å¥½åã€‚è®­ç»ƒé…ç½®ä¹Ÿæ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹å…³é”®è¦ç´ ä¹‹ä¸€ã€‚

é€šè¿‡æ¨¡å‹è®¡ç®—$x_1$è¡¨ç¤ºçš„å½±å“å› ç´ æ‰€å¯¹åº”çš„æˆ¿ä»·åº”è¯¥æ˜¯$z$, ä½†å®é™…æ•°æ®å‘Šè¯‰æˆ‘ä»¬æˆ¿ä»·æ˜¯$y$ã€‚è¿™æ—¶æˆ‘ä»¬éœ€è¦æœ‰æŸç§æŒ‡æ ‡æ¥è¡¡é‡é¢„æµ‹å€¼$z$è·ŸçœŸå®å€¼$y$ä¹‹é—´çš„å·®è·ã€‚å¯¹äºå›å½’é—®é¢˜ï¼Œæœ€å¸¸é‡‡ç”¨çš„è¡¡é‡æ–¹æ³•æ˜¯ä½¿ç”¨å‡æ–¹è¯¯å·®ä½œä¸ºè¯„ä»·æ¨¡å‹å¥½åçš„æŒ‡æ ‡ï¼Œå…·ä½“å®šä¹‰å¦‚ä¸‹ï¼š

$$
Loss = (y - z)^2
$$
ä¸Šå¼ä¸­çš„$Loss$ï¼ˆç®€è®°ä¸º: $L$ï¼‰é€šå¸¸ä¹Ÿè¢«ç§°ä½œæŸå¤±å‡½æ•°ï¼Œå®ƒæ˜¯è¡¡é‡æ¨¡å‹å¥½åçš„æŒ‡æ ‡ã€‚åœ¨å›å½’é—®é¢˜ä¸­å‡æ–¹è¯¯å·®æ˜¯ä¸€ç§æ¯”è¾ƒå¸¸è§çš„å½¢å¼ï¼Œåˆ†ç±»é—®é¢˜ä¸­é€šå¸¸ä¼šé‡‡ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ï¼Œåœ¨åç»­çš„ç« èŠ‚ä¸­ä¼šæ›´è¯¦ç»†çš„ä»‹ç»ã€‚å¯¹ä¸€ä¸ªæ ·æœ¬è®¡ç®—æŸå¤±çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š


```python
Loss = (y1 - z)*(y1 - z)
print(Loss)
```

    [0.39428312]


å› ä¸ºè®¡ç®—æŸå¤±æ—¶éœ€è¦æŠŠæ¯ä¸ªæ ·æœ¬çš„æŸå¤±éƒ½è€ƒè™‘åˆ°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹å•ä¸ªæ ·æœ¬çš„æŸå¤±å‡½æ•°è¿›è¡Œæ±‚å’Œï¼Œå¹¶é™¤ä»¥æ ·æœ¬æ€»æ•°$N$ã€‚
$$L= \frac{1}{N}\sum_i{(y_i - z_i)^2}$$
åœ¨Networkç±»ä¸‹é¢æ·»åŠ æŸå¤±å‡½æ•°çš„è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š


```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        cost = error * error
        cost = np.mean(cost)
        return cost

```

ä½¿ç”¨å®šä¹‰çš„Networkç±»ï¼Œå¯ä»¥æ–¹ä¾¿çš„è®¡ç®—é¢„æµ‹å€¼å’ŒæŸå¤±å‡½æ•°ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç±»ä¸­çš„å˜é‡$x$, $w$ï¼Œ$b$, $z$, $error$ç­‰å‡æ˜¯å‘é‡ã€‚ä»¥å˜é‡$x$ä¸ºä¾‹ï¼Œå…±æœ‰ä¸¤ä¸ªç»´åº¦ï¼Œä¸€ä¸ªä»£è¡¨ç‰¹å¾æ•°é‡ï¼ˆå€¼ä¸º13ï¼‰ï¼Œä¸€ä¸ªä»£è¡¨æ ·æœ¬æ•°é‡ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚


```python
net = Network(13)
# æ­¤å¤„å¯ä»¥ä¸€æ¬¡æ€§è®¡ç®—å¤šä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼å’ŒæŸå¤±å‡½æ•°
x1 = x[0:3]
y1 = y[0:3]
z = net.forward(x1)
print('predict: ', z)
loss = net.loss(z, y1)
print('loss:', loss)
```

    predict:  [[-0.63182506]
     [-0.55793096]
     [-1.00062009]]
    loss: 0.7229825055441156


## è®­ç»ƒè¿‡ç¨‹

ä¸Šè¿°è®¡ç®—è¿‡ç¨‹æè¿°äº†å¦‚ä½•æ„å»ºç¥ç»ç½‘ç»œï¼Œé€šè¿‡ç¥ç»ç½‘ç»œå®Œæˆé¢„æµ‹å€¼å’ŒæŸå¤±å‡½æ•°çš„è®¡ç®—ã€‚æ¥ä¸‹æ¥ä»‹ç»å¦‚ä½•æ±‚è§£å‚æ•°wå’Œbçš„æ•°å€¼ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¹Ÿç§°ä¸ºæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚è®­ç»ƒè¿‡ç¨‹æ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å…³é”®è¦ç´ ä¹‹ä¸€ï¼Œå…¶ç›®æ ‡æ˜¯è®©å®šä¹‰çš„æŸå¤±å‡½æ•°Losså°½å¯èƒ½çš„å°ï¼Œä¹Ÿå°±æ˜¯è¯´æ‰¾åˆ°ä¸€ä¸ªå‚æ•°è§£wå’Œbä½¿å¾—æŸå¤±å‡½æ•°å–å¾—æå°å€¼ã€‚

æˆ‘ä»¬å…ˆåšä¸€ä¸ªå°æµ‹è¯•ï¼šå¦‚ **å›¾5** æ‰€ç¤ºï¼ŒåŸºäºå¾®ç§¯åˆ†çŸ¥è¯†ï¼Œæ±‚ä¸€æ¡æ›²çº¿åœ¨æŸä¸ªç‚¹çš„æ–œç‡ç­‰äºå‡½æ•°è¯¥ç‚¹çš„å¯¼æ•°å€¼ã€‚é‚£ä¹ˆå¤§å®¶æ€è€ƒä¸‹ï¼Œå½“å¤„äºæ›²çº¿çš„æå€¼ç‚¹æ—¶ï¼Œè¯¥ç‚¹çš„æ–œç‡æ˜¯å¤šå°‘ï¼Ÿ

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/94f0437e6a454a0682f3b831c96a62bdaf40898af25145ec9b5b50bc80391f5c" width="300" hegiht="" ></center>
<center><br>å›¾5ï¼šæ›²çº¿æ–œç‡ç­‰äºå¯¼æ•°å€¼</br></center>
<br></br>

è¿™ä¸ªé—®é¢˜å¹¶ä¸éš¾å›ç­”ï¼Œå¤„äºæ›²çº¿æå€¼ç‚¹æ—¶çš„æ–œç‡ä¸º0ï¼Œå³å‡½æ•°åœ¨æå€¼ç‚¹å¤„çš„å¯¼æ•°ä¸º0ã€‚é‚£ä¹ˆï¼Œè®©æŸå¤±å‡½æ•°å–æå°å€¼çš„$w$å’Œ$b$åº”è¯¥æ˜¯ä¸‹è¿°æ–¹ç¨‹ç»„çš„è§£ï¼š
$$
\frac{\partial{L}}{\partial{w}}=0
$$

$$
\frac{\partial{L}}{\partial{b}}=0
$$




å°†æ ·æœ¬æ•°æ®
$$
(x, y)
$$
å¸¦å…¥ä¸Šé¢çš„æ–¹ç¨‹ç»„ä¸­å³å¯æ±‚è§£å‡º$w$å’Œ$b$çš„å€¼ï¼Œä½†æ˜¯è¿™ç§æ–¹æ³•åªå¯¹çº¿æ€§å›å½’è¿™æ ·ç®€å•çš„ä»»åŠ¡æœ‰æ•ˆã€‚å¦‚æœæ¨¡å‹ä¸­å«æœ‰éçº¿æ€§å˜æ¢ï¼Œæˆ–è€…æŸå¤±å‡½æ•°ä¸æ˜¯å‡æ–¹å·®è¿™ç§ç®€å•çš„å½¢å¼ï¼Œåˆ™å¾ˆéš¾é€šè¿‡ä¸Šå¼æ±‚è§£ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¸‹é¢æˆ‘ä»¬å°†å¼•å…¥æ›´åŠ æ™®é€‚çš„æ•°å€¼æ±‚è§£æ–¹æ³•ï¼šæ¢¯åº¦ä¸‹é™æ³•ã€‚

### æ¢¯åº¦ä¸‹é™æ³•

åœ¨ç°å®ä¸­å­˜åœ¨å¤§é‡çš„å‡½æ•°æ­£å‘æ±‚è§£å®¹æ˜“ï¼Œåå‘æ±‚è§£è¾ƒéš¾ï¼Œè¢«ç§°ä¸ºå•å‘å‡½æ•°ã€‚è¿™ç§å‡½æ•°åœ¨å¯†ç å­¦ä¸­æœ‰å¤§é‡çš„åº”ç”¨ï¼Œå¯†ç é”çš„ç‰¹ç‚¹æ˜¯å¯ä»¥è¿…é€Ÿåˆ¤æ–­ä¸€ä¸ªå¯†é’¥æ˜¯å¦æ˜¯æ­£ç¡®çš„(å·²çŸ¥xï¼Œæ±‚yå¾ˆå®¹æ˜“)ï¼Œä½†æ˜¯å³ä½¿è·å–åˆ°å¯†ç é”ç³»ç»Ÿï¼Œæ— æ³•ç ´è§£å‡ºæ­£ç¡®çš„å¯†é’¥æ˜¯ä»€ä¹ˆï¼ˆå·²çŸ¥yï¼Œæ±‚xå¾ˆéš¾ï¼‰ã€‚

è¿™ç§æƒ…å†µç‰¹åˆ«ç±»ä¼¼äºä¸€ä½æƒ³ä»å±±å³°èµ°åˆ°å¡è°·çš„ç›²äººï¼Œä»–çœ‹ä¸è§å¡è°·åœ¨å“ªï¼ˆæ— æ³•é€†å‘æ±‚è§£å‡º$Loss&å¯¼æ•°ä¸º0æ—¶çš„å‚æ•°å€¼ï¼‰ï¼Œä½†å¯ä»¥ä¼¸è„šæ¢ç´¢èº«è¾¹çš„å¡åº¦ï¼ˆå½“å‰ç‚¹çš„å¯¼æ•°å€¼ï¼Œä¹Ÿç§°ä¸ºæ¢¯åº¦ï¼‰ã€‚é‚£ä¹ˆï¼Œæ±‚è§£Losså‡½æ•°æœ€å°å€¼å¯ä»¥â€œä»å½“å‰çš„å‚æ•°å–å€¼ï¼Œä¸€æ­¥æ­¥çš„æŒ‰ç…§ä¸‹å¡çš„æ–¹å‘ä¸‹é™ï¼Œç›´åˆ°èµ°åˆ°æœ€ä½ç‚¹â€å®ç°ã€‚è¿™ç§æ–¹æ³•ç¬”è€…ä¸ªäººç§°å®ƒä¸ºâ€œç›²äººä¸‹å¡æ³•â€ã€‚å“¦ä¸ï¼Œæœ‰ä¸ªæ›´æ­£å¼çš„è¯´æ³•â€œæ¢¯åº¦ä¸‹é™æ³•â€ã€‚

è®­ç»ƒçš„å…³é”®æ˜¯æ‰¾åˆ°ä¸€ç»„(w, b)ï¼Œä½¿å¾—æŸå¤±å‡½$Lå–æå°å€¼ã€‚æˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹æŸå¤±å‡½æ•°Låªéšä¸¤ä¸ªå‚æ•°w_5ã€w_9å˜åŒ–æ—¶çš„ç®€å•æƒ…å½¢ï¼Œå¯å‘ä¸‹å¯»è§£çš„æ€è·¯ã€‚
$$
L=L(w_5, w_9)
$$
è¿™é‡Œæˆ‘ä»¬å°†w_0, w_1, ..., w_{12}ä¸­é™¤w_5, w_9ä¹‹å¤–çš„å‚æ•°å’Œbéƒ½å›ºå®šä¸‹æ¥ï¼Œå¯ä»¥ç”¨å›¾ç”»å‡ºL(w_5, w_9)çš„å½¢å¼ã€‚


```python
net = Network(13)
losses = []
#åªç”»å‡ºå‚æ•°w5å’Œw9åœ¨åŒºé—´[-160, 160]çš„æ›²çº¿éƒ¨åˆ†ï¼Œä»¥åŠåŒ…å«æŸå¤±å‡½æ•°çš„æå€¼
w5 = np.arange(-160.0, 160.0, 1.0)
w9 = np.arange(-160.0, 160.0, 1.0)
losses = np.zeros([len(w5), len(w9)])

#è®¡ç®—è®¾å®šåŒºåŸŸå†…æ¯ä¸ªå‚æ•°å–å€¼æ‰€å¯¹åº”çš„Loss
for i in range(len(w5)):
    for j in range(len(w9)):
        net.w[5] = w5[i]
        net.w[9] = w9[j]
        z = net.forward(x)
        loss = net.loss(z, y)
        losses[i, j] = loss

#ä½¿ç”¨matplotlibå°†ä¸¤ä¸ªå˜é‡å’Œå¯¹åº”çš„Lossä½œ3Då›¾
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = Axes3D(fig)

w5, w9 = np.meshgrid(w5, w9)

ax.plot_surface(w5, w9, losses, rstride=1, cstride=1, cmap='rainbow')
plt.show()
```


![png](output_32_0.png)


å¯¹äºè¿™ç§ç®€å•æƒ…å½¢ï¼Œæˆ‘ä»¬åˆ©ç”¨ä¸Šé¢çš„ç¨‹åºï¼Œå¯ä»¥åœ¨ä¸‰ç»´ç©ºé—´ä¸­ç”»å‡ºæŸå¤±å‡½æ•°éšå‚æ•°å˜åŒ–çš„æ›²é¢å›¾ã€‚ä»å›¾ä¸­å¯ä»¥çœ‹å‡ºæœ‰äº›åŒºåŸŸçš„å‡½æ•°å€¼æ˜æ˜¾æ¯”å‘¨å›´çš„ç‚¹å°ã€‚

éœ€è¦è¯´æ˜çš„æ˜¯ï¼šä¸ºä»€ä¹ˆè¿™é‡Œæˆ‘ä»¬é€‰æ‹©w_5å’Œw_9æ¥ç”»å›¾ï¼Ÿè¿™æ˜¯å› ä¸ºé€‰æ‹©è¿™ä¸¤ä¸ªå‚æ•°çš„æ—¶å€™ï¼Œå¯æ¯”è¾ƒç›´è§‚çš„ä»æŸå¤±å‡½æ•°çš„æ›²é¢å›¾ä¸Šå‘ç°æå€¼ç‚¹çš„å­˜åœ¨ã€‚å…¶ä»–å‚æ•°ç»„åˆï¼Œä»å›¾å½¢ä¸Šè§‚æµ‹æŸå¤±å‡½æ•°çš„æå€¼ç‚¹ä¸å¤Ÿç›´è§‚ã€‚

è§‚å¯Ÿä¸Šè¿°æ›²çº¿å‘ˆç°å‡ºâ€œåœ†æ»‘â€çš„å¡åº¦ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬é€‰æ‹©ä»¥å‡æ–¹è¯¯å·®ä½œä¸ºæŸå¤±å‡½æ•°çš„åŸå› ä¹‹ä¸€ã€‚**å›¾6** å‘ˆç°äº†åªæœ‰ä¸€ä¸ªå‚æ•°ç»´åº¦æ—¶ï¼Œå‡æ–¹è¯¯å·®å’Œç»å¯¹å€¼è¯¯å·®ï¼ˆåªå°†æ¯ä¸ªæ ·æœ¬çš„è¯¯å·®ç´¯åŠ ï¼Œä¸åšå¹³æ–¹å¤„ç†ï¼‰çš„æŸå¤±å‡½æ•°æ›²çº¿å›¾ã€‚

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/98c29a9124f943e3ae20663728292c91da4ea7bdf4c54391843bf09dfa95a372" width="500" hegiht="40" ></center>
<center><br>å›¾6ï¼šå‡æ–¹è¯¯å·®å’Œç»å¯¹å€¼è¯¯å·®æŸå¤±å‡½æ•°æ›²çº¿å›¾</br></center>
<br></br>

ç”±æ­¤å¯è§ï¼Œå‡æ–¹è¯¯å·®è¡¨ç°çš„â€œåœ†æ»‘â€çš„å¡åº¦æœ‰ä¸¤ä¸ªå¥½å¤„ï¼š

* æ›²çº¿çš„æœ€ä½ç‚¹æ˜¯å¯å¯¼çš„ã€‚
* è¶Šæ¥è¿‘æœ€ä½ç‚¹ï¼Œæ›²çº¿çš„å¡åº¦é€æ¸æ”¾ç¼“ï¼Œæœ‰åŠ©ä¸åŸºäºå½“å‰çš„æ¢¯åº¦æ¥åˆ¤æ–­æ¥è¿‘æœ€ä½ç‚¹çš„ç¨‹åº¦ï¼ˆæ˜¯å¦é€æ¸å‡å°‘æ­¥é•¿ï¼Œä»¥å…é”™è¿‡æœ€ä½ç‚¹ï¼‰ã€‚

è€Œè¿™ä¸¤ä¸ªç‰¹æ€§ç»å¯¹å€¼è¯¯å·®æ˜¯ä¸å…·å¤‡çš„ï¼Œè¿™ä¹Ÿæ˜¯æŸå¤±å‡½æ•°çš„è®¾è®¡ä¸ä»…ä»…è¦è€ƒè™‘â€œåˆç†æ€§â€ï¼Œè¿˜è¦è¿½æ±‚â€œæ˜“è§£æ€§â€çš„åŸå› ã€‚

ç°åœ¨æˆ‘ä»¬è¦æ‰¾å‡ºä¸€ç»„[w_5, w_9]çš„å€¼ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æœ€å°ï¼Œå®ç°æ¢¯åº¦ä¸‹é™æ³•çš„æ–¹æ¡ˆå¦‚ä¸‹ï¼š

- æ­¥éª¤1ï¼šéšæœºçš„é€‰ä¸€ç»„åˆå§‹å€¼ï¼Œä¾‹å¦‚ï¼š[w_5, w_9] = [-100.0, -100.0]

- æ­¥éª¤2ï¼šé€‰å–ä¸‹ä¸€ä¸ªç‚¹
  $$
  [w_5^{'} , w_9^{'}]
  $$
  ï¼Œä½¿å¾—$
  $$
  L(w_5^{'} , w_9^{'}) < L(w_5, w_9)
  $$
  

- æ­¥éª¤3ï¼šé‡å¤æ­¥éª¤2ï¼Œç›´åˆ°æŸå¤±å‡½æ•°å‡ ä¹ä¸å†ä¸‹é™ã€‚

å¦‚ä½•é€‰æ‹©
$$
[w_5^{'} , w_9^{'}]
$$
$æ˜¯è‡³å…³é‡è¦çš„ï¼Œç¬¬ä¸€è¦ä¿è¯Læ˜¯ä¸‹é™çš„ï¼Œç¬¬äºŒè¦ä½¿å¾—ä¸‹é™çš„è¶‹åŠ¿å°½å¯èƒ½çš„å¿«ã€‚å¾®ç§¯åˆ†çš„åŸºç¡€çŸ¥è¯†å‘Šè¯‰æˆ‘ä»¬ï¼Œæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ï¼Œæ˜¯å‡½æ•°å€¼ä¸‹é™æœ€å¿«çš„æ–¹å‘ï¼Œå¦‚ **å›¾7** æ‰€ç¤ºã€‚ç®€å•ç†è§£ï¼Œå‡½æ•°åœ¨æŸä¸€ä¸ªç‚¹çš„æ¢¯åº¦æ–¹å‘æ˜¯æ›²çº¿æ–œç‡æœ€å¤§çš„æ–¹å‘ï¼Œä½†æ¢¯åº¦æ–¹å‘æ˜¯å‘ä¸Šçš„ï¼Œæ‰€ä»¥ä¸‹é™æœ€å¿«çš„æ˜¯æ¢¯åº¦çš„åæ–¹å‘ã€‚
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/ae404fa017774f268f0afd67281d0bd54421b29bef7d4740b64f9c8ec7d1712c" width="300" hegiht="40" ></center>
<center><br>å›¾7ï¼šæ¢¯åº¦ä¸‹é™æ–¹å‘ç¤ºæ„å›¾</br></center>
<br></br>

### è®¡ç®—æ¢¯åº¦

ä¸Šé¢æˆ‘ä»¬è®²è¿‡äº†æŸå¤±å‡½æ•°çš„è®¡ç®—æ–¹æ³•ï¼Œè¿™é‡Œç¨å¾®åŠ ä»¥æ”¹å†™ã€‚ä¸ºäº†æ¢¯åº¦è®¡ç®—æ›´åŠ ç®€æ´ï¼Œå¼•å…¥å› å­
$$
\frac{1}{2}
$$
ï¼Œå®šä¹‰æŸå¤±å‡½æ•°å¦‚ä¸‹ï¼š

$$
L= \frac{1}{2N}\sum_{i=1}^N{(y_i - z_i)^2}
$$


å…¶ä¸­$z_i$æ˜¯ç½‘ç»œå¯¹ç¬¬$i$ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ï¼š

$$
z_i = \sum_{j=0}^{12}{x_i^{j}\cdot w_j} + b
$$
æ¢¯åº¦çš„å®šä¹‰ï¼š

$$
ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡ = (\frac{\partial{L}}{\partial{w_0}},\frac{\partial{L}}{\partial{w_1}}, ... ,\frac{\partial{L}}{\partial{w_12}} ,\frac{\partial{L}}{\partial{b}})
$$


å¯ä»¥è®¡ç®—å‡ºLå¯¹wå’Œbçš„åå¯¼æ•°ï¼š

$$
\frac{\partial{L}}{\partial{w_j}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{w_j}}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)x_i^{j}}
$$


$$
\frac{\partial{L}}{\partial{b}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{b}}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)}
$$


ä»å¯¼æ•°çš„è®¡ç®—è¿‡ç¨‹å¯ä»¥çœ‹å‡ºï¼Œå› å­
$$
\frac{1}{2}
$$
è¢«æ¶ˆæ‰äº†ï¼Œè¿™æ˜¯å› ä¸ºäºŒæ¬¡å‡½æ•°æ±‚å¯¼çš„æ—¶å€™ä¼šäº§ç”Ÿå› å­2ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬å°†æŸå¤±å‡½æ•°æ”¹å†™çš„åŸå› ã€‚

ä¸‹é¢æˆ‘ä»¬è€ƒè™‘åªæœ‰ä¸€ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—æ¢¯åº¦ï¼š

$$
L= \frac{1}{2}{(y_i - z_i)^2}
$$


$$
z_1 = {x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_12} + b
$$


å¯ä»¥è®¡ç®—å‡ºï¼š

$$
L= \frac{1}{2}{({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_12} + b - y_1)^2}
$$


å¯ä»¥è®¡ç®—å‡ºLå¯¹wå’Œbçš„åå¯¼æ•°ï¼š

$$
\frac{\partial{L}}{\partial{w_0}} = ({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_12} + b - y_1)\cdot x_1^{0}=({z_1} - {y_1})\cdot x_1^{0}
$$


$$
\frac{\partial{L}}{\partial{b}} = ({x_1^{0}\cdot w_0} + {x_1^{1}\cdot w_1} + ...  + {x_1^{12}\cdot w_12} + b - y_1)\cdot 1 = ({z_1} - {y_1})
$$




å¯ä»¥é€šè¿‡å…·ä½“çš„ç¨‹åºæŸ¥çœ‹æ¯ä¸ªå˜é‡çš„æ•°æ®å’Œç»´åº¦ã€‚


```python
x1 = x[0]
y1 = y[0]
z1 = net.forward(x1)
print('x1 {}, shape {}'.format(x1, x1.shape))
print('y1 {}, shape {}'.format(y1, y1.shape))
print('z1 {}, shape {}'.format(z1, z1.shape))
```

    x1 [-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817
      0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112
     -0.17590923], shape (13,)
    y1 [-0.00390539], shape (1,)
    z1 [-12.05947643], shape (1,)


æŒ‰ä¸Šé¢çš„å…¬å¼ï¼Œå½“åªæœ‰ä¸€ä¸ªæ ·æœ¬æ—¶ï¼Œå¯ä»¥è®¡ç®—æŸä¸ªw_jï¼Œæ¯”å¦‚w_0çš„æ¢¯åº¦ã€‚


```python
gradient_w0 = (z1 - y1) * x1[0]
print('gradient_w0 {}'.format(gradient_w0))
```

    gradient_w0 [0.25875126]


åŒæ ·æˆ‘ä»¬å¯ä»¥è®¡ç®—w_1çš„æ¢¯åº¦ã€‚


```python
gradient_w1 = (z1 - y1) * x1[1]
print('gradient_w1 {}'.format(gradient_w1))
```

    gradient_w1 [-0.45417275]


ä¾æ¬¡è®¡ç®—w_2çš„æ¢¯åº¦ã€‚


```python
gradient_w2= (z1 - y1) * x1[2]
print('gradient_w1 {}'.format(gradient_w2))
```

    gradient_w1 [3.44214394]


èªæ˜çš„è¯»è€…å¯èƒ½å·²ç»æƒ³åˆ°ï¼Œå†™ä¸€ä¸ªforå¾ªç¯å³å¯è®¡ç®—ä»$w_0$åˆ°$w_{12}$çš„æ‰€æœ‰æƒé‡çš„æ¢¯åº¦ï¼Œè¯¥æ–¹æ³•è¯»è€…å¯ä»¥è‡ªè¡Œå®ç°ã€‚

### ä½¿ç”¨Numpyè¿›è¡Œæ¢¯åº¦è®¡ç®—

åŸºäºNumpyå¹¿æ’­æœºåˆ¶ï¼ˆå¯¹å‘é‡å’ŒçŸ©é˜µè®¡ç®—å¦‚åŒå¯¹1ä¸ªå•ä¸€å˜é‡è®¡ç®—ä¸€æ ·ï¼‰ï¼Œå¯ä»¥æ›´å¿«é€Ÿçš„å®ç°æ¢¯åº¦è®¡ç®—ã€‚è®¡ç®—æ¢¯åº¦çš„ä»£ç ä¸­ç›´æ¥ç”¨(z1 - y1) * x1ï¼Œå¾—åˆ°çš„æ˜¯ä¸€ä¸ª13ç»´çš„å‘é‡ï¼Œæ¯ä¸ªåˆ†é‡åˆ†åˆ«ä»£è¡¨è¯¥ç»´åº¦çš„æ¢¯åº¦ã€‚


```python
gradient_w = (z1 - y1) * x1
print('gradient_w_by_sample1 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))
```

    gradient_w_by_sample1 [ 0.25875126 -0.45417275  3.44214394  1.04441828 -0.15548386 -0.55875363
     -0.09591377  0.09232085  3.03465138  1.43234507  3.49642036 -0.62581917
      2.12068622], gradient.shape (13,)


è¾“å…¥æ•°æ®ä¸­æœ‰å¤šä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½å¯¹æ¢¯åº¦æœ‰è´¡çŒ®ã€‚å¦‚ä¸Šä»£ç è®¡ç®—äº†åªæœ‰æ ·æœ¬1æ—¶çš„æ¢¯åº¦å€¼ï¼ŒåŒæ ·çš„è®¡ç®—æ–¹æ³•ä¹Ÿå¯ä»¥è®¡ç®—æ ·æœ¬2å’Œæ ·æœ¬3å¯¹æ¢¯åº¦çš„è´¡çŒ®ã€‚


```python
x2 = x[1]
y2 = y[1]
z2 = net.forward(x2)
gradient_w = (z2 - y2) * x2
print('gradient_w_by_sample2 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))
```

    gradient_w_by_sample2 [ 0.7329239   4.91417754  3.33394253  2.9912385   4.45673435 -0.58146277
     -5.14623287 -2.4894594   7.19011988  7.99471607  0.83100061 -1.79236081
      2.11028056], gradient.shape (13,)



```python
x3 = x[2]
y3 = y[2]
z3 = net.forward(x3)
gradient_w = (z3 - y3) * x3
print('gradient_w_by_sample3 {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))
```

    gradient_w_by_sample3 [ 0.25138584  1.68549775  1.14349809  1.02595515  1.5286008  -1.93302947
      0.4058236  -0.85385157  2.46611579  2.74208162  0.28502219 -0.46695229
      2.39363651], gradient.shape (13,)


å¯èƒ½æœ‰çš„è¯»è€…å†æ¬¡æƒ³åˆ°å¯ä»¥ä½¿ç”¨forå¾ªç¯æŠŠæ¯ä¸ªæ ·æœ¬å¯¹æ¢¯åº¦çš„è´¡çŒ®éƒ½è®¡ç®—å‡ºæ¥ï¼Œç„¶åå†ä½œå¹³å‡ã€‚ä½†æ˜¯æˆ‘ä»¬ä¸éœ€è¦è¿™ä¹ˆåšï¼Œä»ç„¶å¯ä»¥ä½¿ç”¨Numpyçš„çŸ©é˜µæ“ä½œæ¥ç®€åŒ–è¿ç®—ï¼Œå¦‚3ä¸ªæ ·æœ¬çš„æƒ…å†µã€‚


```python
# æ³¨æ„è¿™é‡Œæ˜¯ä¸€æ¬¡å–å‡º3ä¸ªæ ·æœ¬çš„æ•°æ®ï¼Œä¸æ˜¯å–å‡ºç¬¬3ä¸ªæ ·æœ¬
x3samples = x[0:3]
y3samples = y[0:3]
z3samples = net.forward(x3samples)

print('x {}, shape {}'.format(x3samples, x3samples.shape))
print('y {}, shape {}'.format(y3samples, y3samples.shape))
print('z {}, shape {}'.format(z3samples, z3samples.shape))
```

    x [[-0.02146321  0.03767327 -0.28552309 -0.08663366  0.01289726  0.04634817
       0.00795597 -0.00765794 -0.25172191 -0.11881188 -0.29002528  0.0519112
      -0.17590923]
     [-0.02122729 -0.14232673 -0.09655922 -0.08663366 -0.12907805  0.0168406
       0.14904763  0.0721009  -0.20824365 -0.23154675 -0.02406783  0.0519112
      -0.06111894]
     [-0.02122751 -0.14232673 -0.09655922 -0.08663366 -0.12907805  0.1632288
      -0.03426854  0.0721009  -0.20824365 -0.23154675 -0.02406783  0.03943037
      -0.20212336]], shape (3, 13)
    y [[-0.00390539]
     [-0.05723872]
     [ 0.23387239]], shape (3, 1)
    z [[-12.05947643]
     [-34.58467747]
     [-11.60858134]], shape (3, 1)


ä¸Šé¢çš„x3samples, y3samples, z3samplesçš„ç¬¬ä¸€ç»´å¤§å°å‡ä¸º3ï¼Œè¡¨ç¤ºæœ‰3ä¸ªæ ·æœ¬ã€‚ä¸‹é¢è®¡ç®—è¿™3ä¸ªæ ·æœ¬å¯¹æ¢¯åº¦çš„è´¡çŒ®ã€‚


```python
gradient_w = (z3samples - y3samples) * x3samples
print('gradient_w {}, gradient.shape {}'.format(gradient_w, gradient_w.shape))
```

    gradient_w [[ 0.25875126 -0.45417275  3.44214394  1.04441828 -0.15548386 -0.55875363
      -0.09591377  0.09232085  3.03465138  1.43234507  3.49642036 -0.62581917
       2.12068622]
     [ 0.7329239   4.91417754  3.33394253  2.9912385   4.45673435 -0.58146277
      -5.14623287 -2.4894594   7.19011988  7.99471607  0.83100061 -1.79236081
       2.11028056]
     [ 0.25138584  1.68549775  1.14349809  1.02595515  1.5286008  -1.93302947
       0.4058236  -0.85385157  2.46611579  2.74208162  0.28502219 -0.46695229
       2.39363651]], gradient.shape (3, 13)


æ­¤å¤„å¯è§ï¼Œè®¡ç®—æ¢¯åº¦gradient_wçš„ç»´åº¦æ˜¯$3 \times 13$ï¼Œå¹¶ä¸”å…¶ç¬¬1è¡Œä¸ä¸Šé¢ç¬¬1ä¸ªæ ·æœ¬è®¡ç®—çš„æ¢¯åº¦gradient_w_by_sample1ä¸€è‡´ï¼Œç¬¬2è¡Œä¸ä¸Šé¢ç¬¬2ä¸ªæ ·æœ¬è®¡ç®—çš„æ¢¯åº¦gradient_w_by_sample1ä¸€è‡´ï¼Œç¬¬3è¡Œä¸ä¸Šé¢ç¬¬3ä¸ªæ ·æœ¬è®¡ç®—çš„æ¢¯åº¦gradient_w_by_sample1ä¸€è‡´ã€‚è¿™é‡Œä½¿ç”¨çŸ©é˜µæ“ä½œï¼Œå¯èƒ½æ›´åŠ æ–¹ä¾¿çš„å¯¹3ä¸ªæ ·æœ¬åˆ†åˆ«è®¡ç®—å„è‡ªå¯¹æ¢¯åº¦çš„è´¡çŒ®ã€‚

é‚£ä¹ˆå¯¹äºæœ‰Nä¸ªæ ·æœ¬çš„æƒ…å½¢ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨å¦‚ä¸‹æ–¹å¼è®¡ç®—å‡ºæ‰€æœ‰æ ·æœ¬å¯¹æ¢¯åº¦çš„è´¡çŒ®ï¼Œè¿™å°±æ˜¯ä½¿ç”¨Numpyåº“å¹¿æ’­åŠŸèƒ½å¸¦æ¥çš„ä¾¿æ·ã€‚
å°ç»“ä¸€ä¸‹è¿™é‡Œä½¿ç”¨Numpyåº“çš„å¹¿æ’­åŠŸèƒ½ï¼š
- ä¸€æ–¹é¢å¯ä»¥æ‰©å±•å‚æ•°çš„ç»´åº¦ï¼Œä»£æ›¿forå¾ªç¯æ¥è®¡ç®—1ä¸ªæ ·æœ¬å¯¹ä»w0 åˆ°w12 çš„æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚
- å¦ä¸€æ–¹é¢å¯ä»¥æ‰©å±•æ ·æœ¬çš„ç»´åº¦ï¼Œä»£æ›¿forå¾ªç¯æ¥è®¡ç®—æ ·æœ¬0åˆ°æ ·æœ¬403å¯¹å‚æ•°çš„æ¢¯åº¦ã€‚


```python
z = net.forward(x)
gradient_w = (z - y) * x
print('gradient_w shape {}'.format(gradient_w.shape))
print(gradient_w)
```

    gradient_w shape (404, 13)
    [[  0.25875126  -0.45417275   3.44214394 ...   3.49642036  -0.62581917
        2.12068622]
     [  0.7329239    4.91417754   3.33394253 ...   0.83100061  -1.79236081
        2.11028056]
     [  0.25138584   1.68549775   1.14349809 ...   0.28502219  -0.46695229
        2.39363651]
     ...
     [ 14.70025543 -15.10890735  36.23258734 ...  24.54882966   5.51071122
       26.26098922]
     [  9.29832217 -15.33146159  36.76629344 ...  24.91043398  -1.27564923
       26.61808955]
     [ 19.55115919 -10.8177237   25.94192351 ...  17.5765494    3.94557661
       17.64891012]]


ä¸Šé¢gradient_wçš„æ¯ä¸€è¡Œä»£è¡¨äº†ä¸€ä¸ªæ ·æœ¬å¯¹æ¢¯åº¦çš„è´¡çŒ®ã€‚æ ¹æ®æ¢¯åº¦çš„è®¡ç®—å…¬å¼ï¼Œæ€»æ¢¯åº¦æ˜¯å¯¹æ¯ä¸ªæ ·æœ¬å¯¹æ¢¯åº¦è´¡çŒ®çš„å¹³å‡å€¼ã€‚

$$
\frac{\partial{L}}{\partial{w_j}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)\frac{\partial{z_i}}{\partial{w_j}}} = \frac{1}{N}\sum_{i=1}^N{(z_i - y_i)x_i^{j}}
$$


æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨Numpyçš„å‡å€¼å‡½æ•°æ¥å®Œæˆæ­¤è¿‡ç¨‹ï¼š


```python
# axis = 0 è¡¨ç¤ºæŠŠæ¯ä¸€è¡Œåšç›¸åŠ ç„¶åå†é™¤ä»¥æ€»çš„è¡Œæ•°
gradient_w = np.mean(gradient_w, axis=0)
print('gradient_w ', gradient_w.shape)
print('w ', net.w.shape)
print(gradient_w)
print(net.w)

```

    gradient_w  (13,)
    w  (13, 1)
    [ 1.59697064 -0.92928123  4.72726926  1.65712204  4.96176389  1.18068454
      4.55846519 -3.37770889  9.57465893 10.29870662  1.3900257  -0.30152215
      1.09276043]
    [[ 1.76405235e+00]
     [ 4.00157208e-01]
     [ 9.78737984e-01]
     [ 2.24089320e+00]
     [ 1.86755799e+00]
     [ 1.59000000e+02]
     [ 9.50088418e-01]
     [-1.51357208e-01]
     [-1.03218852e-01]
     [ 1.59000000e+02]
     [ 1.44043571e-01]
     [ 1.45427351e+00]
     [ 7.61037725e-01]]


æˆ‘ä»¬ä½¿ç”¨numpyçš„çŸ©é˜µæ“ä½œæ–¹ä¾¿çš„å®Œæˆäº†gradientçš„è®¡ç®—ï¼Œä½†å¼•å…¥äº†ä¸€ä¸ªé—®é¢˜ï¼Œgradient_wçš„å½¢çŠ¶æ˜¯(13,)ï¼Œè€Œwçš„ç»´åº¦æ˜¯(13, 1)ã€‚å¯¼è‡´è¯¥é—®é¢˜çš„åŸå› æ˜¯ä½¿ç”¨np.meanå‡½æ•°çš„æ—¶å€™æ¶ˆé™¤äº†ç¬¬0ç»´ã€‚ä¸ºäº†åŠ å‡ä¹˜é™¤ç­‰è®¡ç®—æ–¹ä¾¿ï¼Œgradient_wå’Œwå¿…é¡»ä¿æŒä¸€è‡´çš„å½¢çŠ¶ã€‚å› æ­¤æˆ‘ä»¬å°†gradient_wçš„ç»´åº¦ä¹Ÿè®¾ç½®ä¸º(13, 1)ï¼Œä»£ç å¦‚ä¸‹ï¼š


```python
gradient_w = gradient_w[:, np.newaxis]
print('gradient_w shape', gradient_w.shape)
```

    gradient_w shape (13, 1)


ç»¼åˆä¸Šé¢çš„è®¨è®ºï¼Œè®¡ç®—æ¢¯åº¦çš„ä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚


```python
z = net.forward(x)
gradient_w = (z - y) * x
gradient_w = np.mean(gradient_w, axis=0)
gradient_w = gradient_w[:, np.newaxis]
gradient_w
```




    array([[ 1.59697064],
           [-0.92928123],
           [ 4.72726926],
           [ 1.65712204],
           [ 4.96176389],
           [ 1.18068454],
           [ 4.55846519],
           [-3.37770889],
           [ 9.57465893],
           [10.29870662],
           [ 1.3900257 ],
           [-0.30152215],
           [ 1.09276043]])



ä¸Šè¿°ä»£ç éå¸¸ç®€æ´çš„å®Œæˆäº†wçš„æ¢¯åº¦è®¡ç®—ã€‚åŒæ ·ï¼Œè®¡ç®—$b$çš„æ¢¯åº¦çš„ä»£ç ä¹Ÿæ˜¯ç±»ä¼¼çš„åŸç†ã€‚


```python
gradient_b = (z - y)
gradient_b = np.mean(gradient_b)
# æ­¤å¤„bæ˜¯ä¸€ä¸ªæ•°å€¼ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥ç”¨np.meanå¾—åˆ°ä¸€ä¸ªæ ‡é‡
gradient_b
```




    -1.0918438870293816e-13



å°†ä¸Šé¢è®¡ç®—wå’Œbçš„æ¢¯åº¦çš„è¿‡ç¨‹ï¼Œå†™æˆNetworkç±»çš„gradientå‡½æ•°ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚


```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = (z - y)
        gradient_b = np.mean(gradient_b)
        
        return gradient_w, gradient_b
```


```python
# è°ƒç”¨ä¸Šé¢å®šä¹‰çš„gradientå‡½æ•°ï¼Œè®¡ç®—æ¢¯åº¦
# åˆå§‹åŒ–ç½‘ç»œï¼Œ
net = Network(13)
# è®¾ç½®[w5, w9] = [-100., +100.]
net.w[5] = -100.0
net.w[9] = -100.0

z = net.forward(x)
loss = net.loss(z, y)
gradient_w, gradient_b = net.gradient(x, y)
gradient_w5 = gradient_w[5][0]
gradient_w9 = gradient_w[9][0]
print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))
print('gradient {}'.format([gradient_w5, gradient_w9]))

```

    point [-100.0, -100.0], loss 686.3005008179159
    gradient [-0.850073323995813, -6.138412364807849]


### ç¡®å®šæŸå¤±å‡½æ•°æ›´å°çš„ç‚¹

ä¸‹é¢æˆ‘ä»¬å¼€å§‹ç ”ç©¶æ›´æ–°æ¢¯åº¦çš„æ–¹æ³•ã€‚é¦–å…ˆæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ç§»åŠ¨ä¸€å°æ­¥ï¼Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªç‚¹P1ï¼Œè§‚å¯ŸæŸå¤±å‡½æ•°çš„å˜åŒ–ã€‚


```python
# åœ¨[w5, w9]å¹³é¢ä¸Šï¼Œæ²¿ç€æ¢¯åº¦çš„åæ–¹å‘ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªç‚¹P1
# å®šä¹‰ç§»åŠ¨æ­¥é•¿ eta
eta = 0.1
# æ›´æ–°å‚æ•°w5å’Œw9
net.w[5] = net.w[5] - eta * gradient_w5
net.w[9] = net.w[9] - eta * gradient_w9
# é‡æ–°è®¡ç®—zå’Œloss
z = net.forward(x)
loss = net.loss(z, y)
gradient_w, gradient_b = net.gradient(x, y)
gradient_w5 = gradient_w[5][0]
gradient_w9 = gradient_w[9][0]
print('point {}, loss {}'.format([net.w[5][0], net.w[9][0]], loss))
print('gradient {}'.format([gradient_w5, gradient_w9]))
```

    point [-99.91499266760042, -99.38615876351922], loss 678.6472185028845
    gradient [-0.8556356178645292, -6.0932268634065805]


è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œå¯ä»¥å‘ç°æ²¿ç€æ¢¯åº¦åæ–¹å‘èµ°ä¸€å°æ­¥ï¼Œä¸‹ä¸€ä¸ªç‚¹çš„æŸå¤±å‡½æ•°çš„ç¡®å‡å°‘äº†ã€‚æ„Ÿå…´è¶£çš„è¯ï¼Œå¤§å®¶å¯ä»¥å°è¯•ä¸åœçš„ç‚¹å‡»ä¸Šé¢çš„ä»£ç å—ï¼Œè§‚å¯ŸæŸå¤±å‡½æ•°æ˜¯å¦ä¸€ç›´åœ¨å˜å°ã€‚

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œæ¯æ¬¡æ›´æ–°å‚æ•°ä½¿ç”¨çš„è¯­å¥ï¼š
net.w[5] = net.w[5] - eta * gradient_w5

* ç›¸å‡ï¼šå‚æ•°éœ€è¦å‘æ¢¯åº¦çš„åæ–¹å‘ç§»åŠ¨ã€‚
* etaï¼šæ§åˆ¶æ¯æ¬¡å‚æ•°å€¼æ²¿ç€æ¢¯åº¦åæ–¹å‘å˜åŠ¨çš„å¤§å°ï¼Œå³æ¯æ¬¡ç§»åŠ¨çš„æ­¥é•¿ï¼Œåˆç§°ä¸ºå­¦ä¹ ç‡ã€‚

å¤§å®¶å¯ä»¥æ€è€ƒä¸‹ï¼Œä¸ºä»€ä¹ˆä¹‹å‰æˆ‘ä»¬è¦åšè¾“å…¥ç‰¹å¾çš„å½’ä¸€åŒ–ï¼Œä¿æŒå°ºåº¦ä¸€è‡´ï¼Ÿè¿™æ˜¯ä¸ºäº†è®©ç»Ÿä¸€çš„æ­¥é•¿æ›´åŠ åˆé€‚ã€‚

å¦‚ **å›¾8** æ‰€ç¤ºï¼Œç‰¹å¾è¾“å…¥å½’ä¸€åŒ–åï¼Œä¸åŒå‚æ•°è¾“å‡ºçš„Lossæ˜¯ä¸€ä¸ªæ¯”è¾ƒè§„æ•´çš„æ›²çº¿ï¼Œå­¦ä¹ ç‡å¯ä»¥è®¾ç½®æˆç»Ÿä¸€çš„å€¼ ï¼›ç‰¹å¾è¾“å…¥æœªå½’ä¸€åŒ–æ—¶ï¼Œä¸åŒç‰¹å¾å¯¹åº”çš„å‚æ•°æ‰€éœ€çš„æ­¥é•¿ä¸ä¸€è‡´ï¼Œå°ºåº¦è¾ƒå¤§çš„å‚æ•°éœ€è¦å¤§æ­¥é•¿ï¼Œå°ºå¯¸è¾ƒå°çš„å‚æ•°éœ€è¦å°æ­¥é•¿ï¼Œå¯¼è‡´æ— æ³•è®¾ç½®ç»Ÿä¸€çš„å­¦ä¹ ç‡ã€‚

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/929dd42cefc349788a5173f47e13f2c81147d6179283477ea563f66e24517987" width="300" hegiht="40" ></center>
<center><br>å›¾8ï¼šæœªå½’ä¸€åŒ–çš„ç‰¹å¾ï¼Œä¼šå¯¼è‡´ä¸åŒç‰¹å¾ç»´åº¦çš„ç†æƒ³æ­¥é•¿ä¸åŒ</br></center>
<br></br>

###  ä»£ç å°è£…Trainå‡½æ•°

å°†ä¸Šé¢çš„å¾ªç¯çš„è®¡ç®—è¿‡ç¨‹å°è£…åœ¨trainå’Œupdateå‡½æ•°ä¸­ï¼Œä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚


```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights,1)
        self.w[5] = -100.
        self.w[9] = -100.
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = (z - y)
        gradient_b = np.mean(gradient_b)        
        return gradient_w, gradient_b
    
    def update(self, graident_w5, gradient_w9, eta=0.01):
        net.w[5] = net.w[5] - eta * gradient_w5
        net.w[9] = net.w[9] - eta * gradient_w9
        
    def train(self, x, y, iterations=100, eta=0.01):
        points = []
        losses = []
        for i in range(iterations):
            points.append([net.w[5][0], net.w[9][0]])
            z = self.forward(x)
            L = self.loss(z, y)
            gradient_w, gradient_b = self.gradient(x, y)
            gradient_w5 = gradient_w[5][0]
            gradient_w9 = gradient_w[9][0]
            self.update(gradient_w5, gradient_w9, eta)
            losses.append(L)
            if i % 50 == 0:
                print('iter {}, point {}, loss {}'.format(i, [net.w[5][0], net.w[9][0]], L))
        return points, losses

# è·å–æ•°æ®
train_data, test_data = load_data()
x = train_data[:, :-1]
y = train_data[:, -1:]
# åˆ›å»ºç½‘ç»œ
net = Network(13)
num_iterations=2000
# å¯åŠ¨è®­ç»ƒ
points, losses = net.train(x, y, iterations=num_iterations, eta=0.01)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(num_iterations)
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

    iter 0, point [-99.99144364382136, -99.93861587635192], loss 686.3005008179159
    iter 50, point [-99.56362583488914, -96.92631128470325], loss 649.221346830939
    iter 100, point [-99.13580802595692, -94.02279509580971], loss 614.6970095624063
    iter 150, point [-98.7079902170247, -91.22404911807594], loss 582.543755023494
    iter 200, point [-98.28017240809248, -88.52620357520894], loss 552.5911329872217
    iter 250, point [-97.85235459916026, -85.9255316243737], loss 524.6810152322887
    iter 300, point [-97.42453679022805, -83.41844407682491], loss 498.6667034691001
    iter 350, point [-96.99671898129583, -81.00148431353688], loss 474.4121018974464
    iter 400, point [-96.56890117236361, -78.67132338862874], loss 451.7909497114133
    iter 450, point [-96.14108336343139, -76.42475531364933], loss 430.68610920670284
    iter 500, point [-95.71326555449917, -74.25869251604028], loss 410.988905460488
    iter 550, point [-95.28544774556696, -72.17016146534513], loss 392.5985138460824
    iter 600, point [-94.85762993663474, -70.15629846096763], loss 375.4213919156372
    iter 650, point [-94.42981212770252, -68.21434557551346], loss 359.3707524354014
    iter 700, point [-94.0019943187703, -66.34164674796719], loss 344.36607459115214
    iter 750, point [-93.57417650983808, -64.53564402117185], loss 330.33265059761464
    iter 800, point [-93.14635870090586, -62.793873918279786], loss 317.2011651461846
    iter 850, point [-92.71854089197365, -61.11396395304264], loss 304.907305311265
    iter 900, point [-92.29072308304143, -59.49362926899678], loss 293.3913987080144
    iter 950, point [-91.86290527410921, -57.930669402782904], loss 282.5980778542974
    iter 1000, point [-91.43508746517699, -56.4229651670156], loss 272.47596883802515
    iter 1050, point [-91.00726965624477, -54.968475648286564], loss 262.9774025287022
    iter 1100, point [-90.57945184731255, -53.56523531604897], loss 254.05814669965383
    iter 1150, point [-90.15163403838034, -52.21135123828792], loss 245.67715754581488
    iter 1200, point [-89.72381622944812, -50.90500040003218], loss 237.796349191773
    iter 1250, point [-89.2959984205159, -49.6444271209092], loss 230.3803798866218
    iter 1300, point [-88.86818061158368, -48.42794056808474], loss 223.3964536766492
    iter 1350, point [-88.44036280265146, -47.2539123610643], loss 216.81413643451378
    iter 1400, point [-88.01254499371925, -46.12077426496303], loss 210.60518520483126
    iter 1450, point [-87.58472718478703, -45.027015968976976], loss 204.74338990147896
    iter 1500, point [-87.15690937585481, -43.9711829469081], loss 199.20442646183588
    iter 1550, point [-86.72909156692259, -42.95187439671279], loss 193.96572062803054
    iter 1600, point [-86.30127375799037, -41.96774125615467], loss 189.00632158541163
    iter 1650, point [-85.87345594905815, -41.017484291751295], loss 184.3067847442463
    iter 1700, point [-85.44563814012594, -40.0998522583068], loss 179.84906300239203
    iter 1750, point [-85.01782033119372, -39.21364012642417], loss 175.61640587468244
    iter 1800, point [-84.5900025222615, -38.35768737548557], loss 171.59326591927967
    iter 1850, point [-84.16218471332928, -37.530876349682856], loss 167.76521193253296
    iter 1900, point [-83.73436690439706, -36.73213067476985], loss 164.11884842217898
    iter 1950, point [-83.30654909546485, -35.96041373329276], loss 160.64174090423475



![png](output_68_1.png)


### è®­ç»ƒæ‰©å±•åˆ°å…¨éƒ¨å‚æ•°

ä¸ºäº†èƒ½ç»™è¯»è€…ç›´è§‚çš„æ„Ÿå—ï¼Œä¸Šé¢æ¼”ç¤ºçš„æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ä»…åŒ…å«$w_5$å’Œ$w_9$ä¸¤ä¸ªå‚æ•°ï¼Œä½†æˆ¿ä»·é¢„æµ‹çš„å®Œæ•´æ¨¡å‹ï¼Œå¿…é¡»è¦å¯¹æ‰€æœ‰å‚æ•°$w$å’Œ$b$è¿›è¡Œæ±‚è§£ã€‚è¿™éœ€è¦å°†Networkä¸­çš„updateå’Œtrainå‡½æ•°è¿›è¡Œä¿®æ”¹ã€‚ç”±äºä¸å†é™å®šå‚ä¸è®¡ç®—çš„å‚æ•°ï¼ˆæ‰€æœ‰å‚æ•°å‡å‚ä¸è®¡ç®—ï¼‰ï¼Œä¿®æ”¹ä¹‹åçš„ä»£ç åè€Œæ›´åŠ ç®€æ´ã€‚å®ç°é€»è¾‘ï¼šâ€œå‰å‘è®¡ç®—è¾“å‡ºã€æ ¹æ®è¾“å‡ºå’ŒçœŸå®å€¼è®¡ç®—Lossã€åŸºäºLosså’Œè¾“å…¥è®¡ç®—æ¢¯åº¦ã€æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°å€¼â€å››ä¸ªéƒ¨åˆ†åå¤æ‰§è¡Œï¼Œç›´åˆ°åˆ°è¾¾å‚æ•°æœ€ä¼˜ç‚¹ã€‚å…·ä½“ä»£ç å¦‚ä¸‹æ‰€ç¤ºã€‚


```python
class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        gradient_w = (z-y)*x
        gradient_w = np.mean(gradient_w, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = (z - y)
        gradient_b = np.mean(gradient_b)        
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
        
    def train(self, x, y, iterations=100, eta=0.01):
        losses = []
        for i in range(iterations):
            z = self.forward(x)
            L = self.loss(z, y)
            gradient_w, gradient_b = self.gradient(x, y)
            self.update(gradient_w, gradient_b, eta)
            losses.append(L)
            if (i+1) % 10 == 0:
                print('iter {}, loss {}'.format(i, L))
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()
x = train_data[:, :-1]
y = train_data[:, -1:]
# åˆ›å»ºç½‘ç»œ
net = Network(13)
num_iterations=1000
# å¯åŠ¨è®­ç»ƒ
losses = net.train(x,y, iterations=num_iterations, eta=0.01)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(num_iterations)
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

    iter 9, loss 1.8984947314576224
    iter 19, loss 1.8031783384598725
    iter 29, loss 1.7135517565541092
    iter 39, loss 1.6292649416831264
    iter 49, loss 1.5499895293373231
    iter 59, loss 1.4754174896452612
    iter 69, loss 1.4052598659324693
    iter 79, loss 1.3392455915676864
    iter 89, loss 1.2771203802372915
    iter 99, loss 1.218645685090292
    iter 109, loss 1.1635977224791534
    iter 119, loss 1.111766556287068
    iter 129, loss 1.0629552390811503
    iter 139, loss 1.0169790065644477
    iter 149, loss 0.9736645220185994
    iter 159, loss 0.9328491676343147
    iter 169, loss 0.8943803798194307
    iter 179, loss 0.8581150257549611
    iter 189, loss 0.8239188186389669
    iter 199, loss 0.7916657692169988
    iter 209, loss 0.761237671346902
    iter 219, loss 0.7325236194855752
    iter 229, loss 0.7054195561163928
    iter 239, loss 0.6798278472589763
    iter 249, loss 0.6556568843183528
    iter 259, loss 0.6328207106387195
    iter 269, loss 0.6112386712285092
    iter 279, loss 0.59083508421862
    iter 289, loss 0.5715389327049418
    iter 299, loss 0.5532835757100347
    iter 309, loss 0.5360064770773406
    iter 319, loss 0.5196489511849665
    iter 329, loss 0.5041559244351539
    iter 339, loss 0.48947571154034963
    iter 349, loss 0.47555980568755696
    iter 359, loss 0.46236268171965056
    iter 369, loss 0.44984161152579916
    iter 379, loss 0.43795649088328303
    iter 389, loss 0.4266696770400226
    iter 399, loss 0.41594583637124666
    iter 409, loss 0.4057518014851036
    iter 419, loss 0.3960564371908221
    iter 429, loss 0.38683051477942226
    iter 439, loss 0.3780465941011246
    iter 449, loss 0.3696789129556087
    iter 459, loss 0.3617032833413179
    iter 469, loss 0.3540969941381648
    iter 479, loss 0.3468387198244131
    iter 489, loss 0.3399084348532937
    iter 499, loss 0.33328733333814486
    iter 509, loss 0.3269577537166779
    iter 519, loss 0.32090310808539985
    iter 529, loss 0.3151078159144129
    iter 539, loss 0.30955724187078903
    iter 549, loss 0.3042376374955925
    iter 559, loss 0.2991360864954391
    iter 569, loss 0.2942404534243286
    iter 579, loss 0.2895393355454012
    iter 589, loss 0.28502201767532415
    iter 599, loss 0.28067842982626157
    iter 609, loss 0.27649910747186535
    iter 619, loss 0.2724751542744919
    iter 629, loss 0.2685982071209627
    iter 639, loss 0.26486040332365085
    iter 649, loss 0.2612543498525749
    iter 659, loss 0.2577730944725093
    iter 669, loss 0.2544100986669443
    iter 679, loss 0.2511592122380609
    iter 689, loss 0.2480146494787638
    iter 699, loss 0.24497096681926708
    iter 709, loss 0.2420230418567802
    iter 719, loss 0.23916605368251415
    iter 729, loss 0.23639546442555454
    iter 739, loss 0.23370700193813704
    iter 749, loss 0.2310966435515475
    iter 759, loss 0.2285606008362593
    iter 769, loss 0.22609530530403904
    iter 779, loss 0.22369739499361888
    iter 789, loss 0.2213637018851542
    iter 799, loss 0.21909124009208833
    iter 809, loss 0.21687719478222933
    iter 819, loss 0.21471891178284025
    iter 829, loss 0.21261388782734392
    iter 839, loss 0.2105597614038757
    iter 849, loss 0.20855430416838638
    iter 859, loss 0.20659541288730932
    iter 869, loss 0.20468110187697833
    iter 879, loss 0.2028094959090178
    iter 889, loss 0.20097882355283644
    iter 899, loss 0.19918741092814596
    iter 909, loss 0.19743367584210875
    iter 919, loss 0.1957161222872899
    iter 929, loss 0.19403333527807176
    iter 939, loss 0.19238397600456975
    iter 949, loss 0.19076677728439412
    iter 959, loss 0.1891805392938162
    iter 969, loss 0.18762412556104593
    iter 979, loss 0.18609645920539716
    iter 989, loss 0.18459651940712488
    iter 999, loss 0.18312333809366155



![png](output_70_1.png)


### éšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆ Stochastic Gradient Descentï¼‰

åœ¨ä¸Šè¿°ç¨‹åºä¸­ï¼Œæ¯æ¬¡æŸå¤±å‡½æ•°å’Œæ¢¯åº¦è®¡ç®—éƒ½æ˜¯åŸºäºæ•°æ®é›†ä¸­çš„å…¨é‡æ•°æ®ã€‚å¯¹äºæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ä»»åŠ¡æ•°æ®é›†è€Œè¨€ï¼Œæ ·æœ¬æ•°æ¯”è¾ƒå°‘ï¼Œåªæœ‰404ä¸ªã€‚ä½†åœ¨å®é™…é—®é¢˜ä¸­ï¼Œæ•°æ®é›†å¾€å¾€éå¸¸å¤§ï¼Œå¦‚æœæ¯æ¬¡éƒ½ä½¿ç”¨å…¨é‡æ•°æ®è¿›è¡Œè®¡ç®—ï¼Œæ•ˆç‡éå¸¸ä½ï¼Œé€šä¿—çš„è¯´å°±æ˜¯â€œæ€é¸¡ç„‰ç”¨ç‰›åˆ€â€ã€‚ç”±äºå‚æ•°æ¯æ¬¡åªæ²¿ç€æ¢¯åº¦åæ–¹å‘æ›´æ–°ä¸€ç‚¹ç‚¹ï¼Œå› æ­¤æ–¹å‘å¹¶ä¸éœ€è¦é‚£ä¹ˆç²¾ç¡®ã€‚ä¸€ä¸ªåˆç†çš„è§£å†³æ–¹æ¡ˆæ˜¯æ¯æ¬¡ä»æ€»çš„æ•°æ®é›†ä¸­éšæœºæŠ½å–å‡ºå°éƒ¨åˆ†æ•°æ®æ¥ä»£è¡¨æ•´ä½“ï¼ŒåŸºäºè¿™éƒ¨åˆ†æ•°æ®è®¡ç®—æ¢¯åº¦å’ŒæŸå¤±æ¥æ›´æ–°å‚æ•°ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä½œéšæœºæ¢¯åº¦ä¸‹é™æ³•ï¼ˆStochastic Gradient Descentï¼ŒSGDï¼‰ï¼Œæ ¸å¿ƒæ¦‚å¿µå¦‚ä¸‹ï¼š

* min-batchï¼šæ¯æ¬¡è¿­ä»£æ—¶æŠ½å–å‡ºæ¥çš„ä¸€æ‰¹æ•°æ®è¢«ç§°ä¸ºä¸€ä¸ªmin-batchã€‚
* batch_sizeï¼šä¸€ä¸ªmini-batchæ‰€åŒ…å«çš„æ ·æœ¬æ•°ç›®ç§°ä¸ºbatch_sizeã€‚
* epochï¼šå½“ç¨‹åºè¿­ä»£çš„æ—¶å€™ï¼ŒæŒ‰mini-batché€æ¸æŠ½å–å‡ºæ ·æœ¬ï¼Œå½“æŠŠæ•´ä¸ªæ•°æ®é›†éƒ½éå†åˆ°äº†çš„æ—¶å€™ï¼Œåˆ™å®Œæˆäº†ä¸€è½®çš„è®­ç»ƒï¼Œä¹Ÿå«ä¸€ä¸ªepochã€‚å¯åŠ¨è®­ç»ƒæ—¶ï¼Œå¯ä»¥å°†è®­ç»ƒçš„è½®æ•°num_epochså’Œbatch_sizeä½œä¸ºå‚æ•°ä¼ å…¥ã€‚

ä¸‹é¢ç»“åˆç¨‹åºä»‹ç»å…·ä½“çš„å®ç°è¿‡ç¨‹ï¼Œæ¶‰åŠåˆ°æ•°æ®å¤„ç†å’Œè®­ç»ƒè¿‡ç¨‹ä¸¤éƒ¨åˆ†ä»£ç çš„ä¿®æ”¹ã€‚

#### **æ•°æ®å¤„ç†ä»£ç ä¿®æ”¹**

æ•°æ®å¤„ç†éœ€è¦å®ç°æ‹†åˆ†æ•°æ®æ‰¹æ¬¡å’Œæ ·æœ¬ä¹±åºï¼ˆä¸ºäº†å®ç°éšæœºæŠ½æ ·çš„æ•ˆæœï¼‰ä¸¤ä¸ªåŠŸèƒ½ã€‚


```python
# è·å–æ•°æ®
train_data, test_data = load_data()
train_data.shape
```




    (404, 14)



train_dataä¸­ä¸€å…±åŒ…å«404æ¡æ•°æ®ï¼Œå¦‚æœbatch_size=10ï¼Œå³å–å‰0-9å·æ ·æœ¬ä½œä¸ºç¬¬ä¸€ä¸ªmini-batchï¼Œå‘½åtrain_data1ã€‚


```python
train_data1 = train_data[0:10]
train_data1.shape
```




    (10, 14)



ä½¿ç”¨train_data1çš„æ•°æ®ï¼ˆ0-9å·æ ·æœ¬ï¼‰è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç½‘ç»œå‚æ•°ã€‚


```python
net = Network(13)
x = train_data1[:, :-1]
y = train_data1[:, -1:]
loss = net.train(x, y, iterations=1, eta=0.01)
loss
```




    [0.9001866101467375]



å†å–å‡º10-19å·æ ·æœ¬ä½œä¸ºç¬¬äºŒä¸ªmini-batchï¼Œè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç½‘ç»œå‚æ•°ã€‚


```python
train_data2 = train_data[10:19]
x = train_data1[:, :-1]
y = train_data1[:, -1:]
loss = net.train(x, y, iterations=1, eta=0.01)
loss
```




    [0.8903272433979657]



æŒ‰æ­¤æ–¹æ³•ä¸æ–­çš„å–å‡ºæ–°çš„mini-batchï¼Œå¹¶é€æ¸æ›´æ–°ç½‘ç»œå‚æ•°ã€‚

æ¥ä¸‹æ¥ï¼Œå°†train_dataåˆ†æˆå¤§å°ä¸ºbatch_sizeçš„å¤šä¸ªmini_batchï¼Œå¦‚ä¸‹ä»£ç æ‰€ç¤ºï¼šå°†train_dataåˆ†æˆ 
$$
\frac{404}{10} + 1 = 41
$$
 ä¸ª mini_batchäº†ï¼Œå…¶ä¸­å‰40ä¸ªmini_batchï¼Œæ¯ä¸ªå‡å«æœ‰10ä¸ªæ ·æœ¬ï¼Œæœ€åä¸€ä¸ªmini_batchåªå«æœ‰4ä¸ªæ ·æœ¬ã€‚


```python
batch_size = 10
n = len(train_data)
mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]
print('total number of mini_batches is ', len(mini_batches))
print('first mini_batch shape ', mini_batches[0].shape)
print('last mini_batch shape ', mini_batches[-1].shape)
```

    total number of mini_batches is  41
    first mini_batch shape  (10, 14)
    last mini_batch shape  (4, 14)


å¦å¤–ï¼Œæˆ‘ä»¬è¿™é‡Œæ˜¯æŒ‰é¡ºåºå–å‡ºmini_batchçš„ï¼Œè€ŒSGDé‡Œé¢æ˜¯éšæœºçš„æŠ½å–ä¸€éƒ¨åˆ†æ ·æœ¬ä»£è¡¨æ€»ä½“ã€‚ä¸ºäº†å®ç°éšæœºæŠ½æ ·çš„æ•ˆæœï¼Œæˆ‘ä»¬å…ˆå°†train_dataé‡Œé¢çš„æ ·æœ¬é¡ºåºéšæœºæ‰“ä¹±ï¼Œç„¶åå†æŠ½å–mini_batchã€‚éšæœºæ‰“ä¹±æ ·æœ¬é¡ºåºï¼Œéœ€è¦ç”¨åˆ°np.random.shuffleå‡½æ•°ï¼Œä¸‹é¢å…ˆä»‹ç»å®ƒçš„ç”¨æ³•ã€‚

------
**è¯´æ˜ï¼š**

é€šè¿‡å¤§é‡å®éªŒå‘ç°ï¼Œæ¨¡å‹å¯¹æœ€åå‡ºç°çš„æ•°æ®å°è±¡æ›´åŠ æ·±åˆ»ã€‚è®­ç»ƒæ•°æ®å¯¼å…¥åï¼Œè¶Šæ¥è¿‘æ¨¡å‹è®­ç»ƒç»“æŸï¼Œæœ€åå‡ ä¸ªæ‰¹æ¬¡æ•°æ®å¯¹æ¨¡å‹å‚æ•°çš„å½±å“è¶Šå¤§ã€‚ä¸ºäº†é¿å…æ¨¡å‹è®°å¿†å½±å“è®­ç»ƒæ•ˆæœï¼Œéœ€è¦è¿›è¡Œæ ·æœ¬ä¹±åºæ“ä½œã€‚

------


```python
# æ–°å»ºä¸€ä¸ªarray
a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])
print('before shuffle', a)
np.random.shuffle(a)
print('after shuffle', a)
```

    before shuffle [ 1  2  3  4  5  6  7  8  9 10 11 12]
    after shuffle [ 7  2 11  3  8  6 12  1  4  5 10  9]


å¤šæ¬¡è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œå¯ä»¥å‘ç°æ¯æ¬¡æ‰§è¡Œshuffleå‡½æ•°åçš„æ•°å­—é¡ºåºå‡ä¸åŒã€‚
ä¸Šé¢ä¸¾çš„æ˜¯ä¸€ä¸ª1ç»´æ•°ç»„ä¹±åºçš„æ¡ˆä¾‹ï¼Œæˆ‘ä»¬åœ¨è§‚å¯Ÿä¸‹2ç»´æ•°ç»„ä¹±åºåçš„æ•ˆæœã€‚


```python
# æ–°å»ºä¸€ä¸ªarray
a = np.array([1,2,3,4,5,6,7,8,9,10,11,12])
a = a.reshape([6, 2])
print('before shuffle\n', a)
np.random.shuffle(a)
print('after shuffle\n', a)
```

    before shuffle
     [[ 1  2]
     [ 3  4]
     [ 5  6]
     [ 7  8]
     [ 9 10]
     [11 12]]
    after shuffle
     [[ 1  2]
     [ 3  4]
     [ 5  6]
     [ 9 10]
     [11 12]
     [ 7  8]]


è§‚å¯Ÿè¿è¡Œç»“æœå¯å‘ç°ï¼Œæ•°ç»„çš„å…ƒç´ åœ¨ç¬¬0ç»´è¢«éšæœºæ‰“ä¹±ï¼Œä½†ç¬¬1ç»´çš„é¡ºåºä¿æŒä¸å˜ã€‚ä¾‹å¦‚æ•°å­—2ä»ç„¶ç´§æŒ¨åœ¨æ•°å­—1çš„åé¢ï¼Œæ•°å­—8ä»ç„¶ç´§æŒ¨åœ¨æ•°å­—7çš„åé¢ï¼Œè€Œç¬¬äºŒç»´çš„[3, 4]å¹¶ä¸æ’åœ¨[1, 2]çš„åé¢ã€‚å°†è¿™éƒ¨åˆ†å®ç°SGDç®—æ³•çš„ä»£ç é›†æˆåˆ°Networkç±»ä¸­çš„trainå‡½æ•°ä¸­ï¼Œæœ€ç»ˆçš„å®Œæ•´ä»£ç å¦‚ä¸‹ã€‚


```python
# è·å–æ•°æ®
train_data, test_data = load_data()

# æ‰“ä¹±æ ·æœ¬é¡ºåº
np.random.shuffle(train_data)

# å°†train_dataåˆ†æˆå¤šä¸ªmini_batch
batch_size = 10
n = len(train_data)
mini_batches = [train_data[k:k+batch_size] for k in range(0, n, batch_size)]

# åˆ›å»ºç½‘ç»œ
net = Network(13)

# ä¾æ¬¡ä½¿ç”¨æ¯ä¸ªmini_batchçš„æ•°æ®
for mini_batch in mini_batches:
    x = mini_batch[:, :-1]
    y = mini_batch[:, -1:]
    loss = net.train(x, y, iterations=1)
```

#### **è®­ç»ƒè¿‡ç¨‹ä»£ç ä¿®æ”¹**

å°†æ¯ä¸ªéšæœºæŠ½å–çš„mini-batchæ•°æ®è¾“å…¥åˆ°æ¨¡å‹ä¸­ç”¨äºå‚æ•°è®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹çš„æ ¸å¿ƒæ˜¯ä¸¤å±‚å¾ªç¯ï¼š

1. ç¬¬ä¸€å±‚å¾ªç¯ï¼Œä»£è¡¨æ ·æœ¬é›†åˆè¦è¢«è®­ç»ƒéå†å‡ æ¬¡ï¼Œç§°ä¸ºâ€œepochâ€ï¼Œä»£ç å¦‚ä¸‹ï¼š

for epoch_id in range(num_epoches):

2. ç¬¬äºŒå±‚å¾ªç¯ï¼Œä»£è¡¨æ¯æ¬¡éå†æ—¶ï¼Œæ ·æœ¬é›†åˆè¢«æ‹†åˆ†æˆçš„å¤šä¸ªæ‰¹æ¬¡ï¼Œéœ€è¦å…¨éƒ¨æ‰§è¡Œè®­ç»ƒï¼Œç§°ä¸ºâ€œiter (iteration)â€ï¼Œ

ä»£ç å¦‚ä¸‹ï¼šfor iter_id,mini_batch in emumerate(mini_batches):

åœ¨ä¸¤å±‚å¾ªç¯çš„å†…éƒ¨æ˜¯ç»å…¸çš„å››æ­¥è®­ç»ƒæµç¨‹ï¼šå‰å‘è®¡ç®—->è®¡ç®—æŸå¤±->è®¡ç®—æ¢¯åº¦->æ›´æ–°å‚æ•°ï¼Œè¿™ä¸å¤§å®¶ä¹‹å‰æ‰€å­¦æ˜¯ä¸€è‡´çš„ï¼Œä»£ç å¦‚ä¸‹ï¼š

                x = mini_batch[:, :-1]
                y = mini_batch[:, -1:]
                a = self.forward(x)  #å‰å‘è®¡ç®—
                loss = self.loss(a, y)  #è®¡ç®—æŸå¤±
                gradient_w, gradient_b = self.gradient(x, y)  #è®¡ç®—æ¢¯åº¦
                self.update(gradient_w, gradient_b, eta)  #æ›´æ–°å‚æ•°


å°†ä¸¤éƒ¨åˆ†æ”¹å†™çš„ä»£ç é›†æˆåˆ°Networkç±»ä¸­çš„trainå‡½æ•°ä¸­ï¼Œæœ€ç»ˆçš„å®ç°å¦‚ä¸‹ã€‚


```python
import numpy as np

class Network(object):
    def __init__(self, num_of_weights):
        # éšæœºäº§ç”Ÿwçš„åˆå§‹å€¼
        # ä¸ºäº†ä¿æŒç¨‹åºæ¯æ¬¡è¿è¡Œç»“æœçš„ä¸€è‡´æ€§ï¼Œæ­¤å¤„è®¾ç½®å›ºå®šçš„éšæœºæ•°ç§å­
        #np.random.seed(0)
        self.w = np.random.randn(num_of_weights, 1)
        self.b = 0.
        
    def forward(self, x):
        z = np.dot(x, self.w) + self.b
        return z
    
    def loss(self, z, y):
        error = z - y
        num_samples = error.shape[0]
        cost = error * error
        cost = np.sum(cost) / num_samples
        return cost
    
    def gradient(self, x, y):
        z = self.forward(x)
        N = x.shape[0]
        gradient_w = 1. / N * np.sum((z-y) * x, axis=0)
        gradient_w = gradient_w[:, np.newaxis]
        gradient_b = 1. / N * np.sum(z-y)
        return gradient_w, gradient_b
    
    def update(self, gradient_w, gradient_b, eta = 0.01):
        self.w = self.w - eta * gradient_w
        self.b = self.b - eta * gradient_b
            
                
    def train(self, training_data, num_epoches, batch_size=10, eta=0.01):
        n = len(training_data)
        losses = []
        for epoch_id in range(num_epoches):
            # åœ¨æ¯è½®è¿­ä»£å¼€å§‹ä¹‹å‰ï¼Œå°†è®­ç»ƒæ•°æ®çš„é¡ºåºéšæœºçš„æ‰“ä¹±ï¼Œ
            # ç„¶åå†æŒ‰æ¯æ¬¡å–batch_sizeæ¡æ•°æ®çš„æ–¹å¼å–å‡º
            np.random.shuffle(training_data)
            # å°†è®­ç»ƒæ•°æ®è¿›è¡Œæ‹†åˆ†ï¼Œæ¯ä¸ªmini_batchåŒ…å«batch_sizeæ¡çš„æ•°æ®
            mini_batches = [training_data[k:k+batch_size] for k in range(0, n, batch_size)]
            for iter_id, mini_batch in enumerate(mini_batches):
                #print(self.w.shape)
                #print(self.b)
                x = mini_batch[:, :-1]
                y = mini_batch[:, -1:]
                a = self.forward(x)
                loss = self.loss(a, y)
                gradient_w, gradient_b = self.gradient(x, y)
                self.update(gradient_w, gradient_b, eta)
                losses.append(loss)
                print('Epoch {:3d} / iter {:3d}, loss = {:.4f}'.
                                 format(epoch_id, iter_id, loss))
        
        return losses

# è·å–æ•°æ®
train_data, test_data = load_data()

# åˆ›å»ºç½‘ç»œ
net = Network(13)
# å¯åŠ¨è®­ç»ƒ
losses = net.train(train_data, num_epoches=50, batch_size=100, eta=0.1)

# ç”»å‡ºæŸå¤±å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿
plot_x = np.arange(len(losses))
plot_y = np.array(losses)
plt.plot(plot_x, plot_y)
plt.show()
```

    Epoch   0 / iter   0, loss = 0.6273
    Epoch   0 / iter   1, loss = 0.4835
    Epoch   0 / iter   2, loss = 0.5830
    Epoch   0 / iter   3, loss = 0.5466
    Epoch   0 / iter   4, loss = 0.2147
    Epoch   1 / iter   0, loss = 0.6645
    Epoch   1 / iter   1, loss = 0.4875
    Epoch   1 / iter   2, loss = 0.4707
    Epoch   1 / iter   3, loss = 0.4153
    Epoch   1 / iter   4, loss = 0.1402
    Epoch   2 / iter   0, loss = 0.5897
    Epoch   2 / iter   1, loss = 0.4373
    Epoch   2 / iter   2, loss = 0.4631
    Epoch   2 / iter   3, loss = 0.3960
    Epoch   2 / iter   4, loss = 0.2340
    Epoch   3 / iter   0, loss = 0.4139
    Epoch   3 / iter   1, loss = 0.5635
    Epoch   3 / iter   2, loss = 0.3807
    Epoch   3 / iter   3, loss = 0.3975
    Epoch   3 / iter   4, loss = 0.1207
    Epoch   4 / iter   0, loss = 0.3786
    Epoch   4 / iter   1, loss = 0.4474
    Epoch   4 / iter   2, loss = 0.4019
    Epoch   4 / iter   3, loss = 0.4352
    Epoch   4 / iter   4, loss = 0.0435
    Epoch   5 / iter   0, loss = 0.4387
    Epoch   5 / iter   1, loss = 0.3886
    Epoch   5 / iter   2, loss = 0.3182
    Epoch   5 / iter   3, loss = 0.4189
    Epoch   5 / iter   4, loss = 0.1741
    Epoch   6 / iter   0, loss = 0.3191
    Epoch   6 / iter   1, loss = 0.3601
    Epoch   6 / iter   2, loss = 0.4199
    Epoch   6 / iter   3, loss = 0.3289
    Epoch   6 / iter   4, loss = 1.2691
    Epoch   7 / iter   0, loss = 0.3202
    Epoch   7 / iter   1, loss = 0.2855
    Epoch   7 / iter   2, loss = 0.4129
    Epoch   7 / iter   3, loss = 0.3331
    Epoch   7 / iter   4, loss = 0.2218
    Epoch   8 / iter   0, loss = 0.2368
    Epoch   8 / iter   1, loss = 0.3457
    Epoch   8 / iter   2, loss = 0.3339
    Epoch   8 / iter   3, loss = 0.3812
    Epoch   8 / iter   4, loss = 0.0534
    Epoch   9 / iter   0, loss = 0.3567
    Epoch   9 / iter   1, loss = 0.4033
    Epoch   9 / iter   2, loss = 0.1926
    Epoch   9 / iter   3, loss = 0.2803
    Epoch   9 / iter   4, loss = 0.1557
    Epoch  10 / iter   0, loss = 0.3435
    Epoch  10 / iter   1, loss = 0.2790
    Epoch  10 / iter   2, loss = 0.3456
    Epoch  10 / iter   3, loss = 0.2076
    Epoch  10 / iter   4, loss = 0.0935
    Epoch  11 / iter   0, loss = 0.3024
    Epoch  11 / iter   1, loss = 0.2517
    Epoch  11 / iter   2, loss = 0.2797
    Epoch  11 / iter   3, loss = 0.2989
    Epoch  11 / iter   4, loss = 0.0301
    Epoch  12 / iter   0, loss = 0.2507
    Epoch  12 / iter   1, loss = 0.2563
    Epoch  12 / iter   2, loss = 0.2971
    Epoch  12 / iter   3, loss = 0.2833
    Epoch  12 / iter   4, loss = 0.0597
    Epoch  13 / iter   0, loss = 0.2827
    Epoch  13 / iter   1, loss = 0.2094
    Epoch  13 / iter   2, loss = 0.2417
    Epoch  13 / iter   3, loss = 0.2985
    Epoch  13 / iter   4, loss = 0.4036
    Epoch  14 / iter   0, loss = 0.3085
    Epoch  14 / iter   1, loss = 0.2015
    Epoch  14 / iter   2, loss = 0.1830
    Epoch  14 / iter   3, loss = 0.2978
    Epoch  14 / iter   4, loss = 0.0630
    Epoch  15 / iter   0, loss = 0.2342
    Epoch  15 / iter   1, loss = 0.2780
    Epoch  15 / iter   2, loss = 0.2571
    Epoch  15 / iter   3, loss = 0.1838
    Epoch  15 / iter   4, loss = 0.0627
    Epoch  16 / iter   0, loss = 0.1896
    Epoch  16 / iter   1, loss = 0.1966
    Epoch  16 / iter   2, loss = 0.2018
    Epoch  16 / iter   3, loss = 0.3257
    Epoch  16 / iter   4, loss = 0.1268
    Epoch  17 / iter   0, loss = 0.1990
    Epoch  17 / iter   1, loss = 0.2031
    Epoch  17 / iter   2, loss = 0.2662
    Epoch  17 / iter   3, loss = 0.2128
    Epoch  17 / iter   4, loss = 0.0133
    Epoch  18 / iter   0, loss = 0.1780
    Epoch  18 / iter   1, loss = 0.1575
    Epoch  18 / iter   2, loss = 0.2547
    Epoch  18 / iter   3, loss = 0.2544
    Epoch  18 / iter   4, loss = 0.2007
    Epoch  19 / iter   0, loss = 0.1657
    Epoch  19 / iter   1, loss = 0.2000
    Epoch  19 / iter   2, loss = 0.2045
    Epoch  19 / iter   3, loss = 0.2524
    Epoch  19 / iter   4, loss = 0.0632
    Epoch  20 / iter   0, loss = 0.1629
    Epoch  20 / iter   1, loss = 0.1895
    Epoch  20 / iter   2, loss = 0.2523
    Epoch  20 / iter   3, loss = 0.1896
    Epoch  20 / iter   4, loss = 0.0918
    Epoch  21 / iter   0, loss = 0.1583
    Epoch  21 / iter   1, loss = 0.2322
    Epoch  21 / iter   2, loss = 0.1567
    Epoch  21 / iter   3, loss = 0.2089
    Epoch  21 / iter   4, loss = 0.2035
    Epoch  22 / iter   0, loss = 0.2273
    Epoch  22 / iter   1, loss = 0.1427
    Epoch  22 / iter   2, loss = 0.1712
    Epoch  22 / iter   3, loss = 0.1826
    Epoch  22 / iter   4, loss = 0.2878
    Epoch  23 / iter   0, loss = 0.1685
    Epoch  23 / iter   1, loss = 0.1622
    Epoch  23 / iter   2, loss = 0.1499
    Epoch  23 / iter   3, loss = 0.2329
    Epoch  23 / iter   4, loss = 0.1486
    Epoch  24 / iter   0, loss = 0.1617
    Epoch  24 / iter   1, loss = 0.2083
    Epoch  24 / iter   2, loss = 0.1442
    Epoch  24 / iter   3, loss = 0.1740
    Epoch  24 / iter   4, loss = 0.1641
    Epoch  25 / iter   0, loss = 0.1159
    Epoch  25 / iter   1, loss = 0.2064
    Epoch  25 / iter   2, loss = 0.1690
    Epoch  25 / iter   3, loss = 0.1778
    Epoch  25 / iter   4, loss = 0.0159
    Epoch  26 / iter   0, loss = 0.1730
    Epoch  26 / iter   1, loss = 0.1861
    Epoch  26 / iter   2, loss = 0.1387
    Epoch  26 / iter   3, loss = 0.1486
    Epoch  26 / iter   4, loss = 0.1090
    Epoch  27 / iter   0, loss = 0.1393
    Epoch  27 / iter   1, loss = 0.1775
    Epoch  27 / iter   2, loss = 0.1564
    Epoch  27 / iter   3, loss = 0.1245
    Epoch  27 / iter   4, loss = 0.7611
    Epoch  28 / iter   0, loss = 0.1470
    Epoch  28 / iter   1, loss = 0.1211
    Epoch  28 / iter   2, loss = 0.1285
    Epoch  28 / iter   3, loss = 0.1854
    Epoch  28 / iter   4, loss = 0.5240
    Epoch  29 / iter   0, loss = 0.1740
    Epoch  29 / iter   1, loss = 0.0898
    Epoch  29 / iter   2, loss = 0.1392
    Epoch  29 / iter   3, loss = 0.1842
    Epoch  29 / iter   4, loss = 0.0251
    Epoch  30 / iter   0, loss = 0.0978
    Epoch  30 / iter   1, loss = 0.1529
    Epoch  30 / iter   2, loss = 0.1640
    Epoch  30 / iter   3, loss = 0.1503
    Epoch  30 / iter   4, loss = 0.0975
    Epoch  31 / iter   0, loss = 0.1399
    Epoch  31 / iter   1, loss = 0.1595
    Epoch  31 / iter   2, loss = 0.1209
    Epoch  31 / iter   3, loss = 0.1203
    Epoch  31 / iter   4, loss = 0.2008
    Epoch  32 / iter   0, loss = 0.1501
    Epoch  32 / iter   1, loss = 0.1310
    Epoch  32 / iter   2, loss = 0.1065
    Epoch  32 / iter   3, loss = 0.1489
    Epoch  32 / iter   4, loss = 0.0818
    Epoch  33 / iter   0, loss = 0.1401
    Epoch  33 / iter   1, loss = 0.1367
    Epoch  33 / iter   2, loss = 0.0970
    Epoch  33 / iter   3, loss = 0.1481
    Epoch  33 / iter   4, loss = 0.0711
    Epoch  34 / iter   0, loss = 0.1157
    Epoch  34 / iter   1, loss = 0.1050
    Epoch  34 / iter   2, loss = 0.1378
    Epoch  34 / iter   3, loss = 0.1505
    Epoch  34 / iter   4, loss = 0.0429
    Epoch  35 / iter   0, loss = 0.1096
    Epoch  35 / iter   1, loss = 0.1279
    Epoch  35 / iter   2, loss = 0.1715
    Epoch  35 / iter   3, loss = 0.0888
    Epoch  35 / iter   4, loss = 0.0473
    Epoch  36 / iter   0, loss = 0.1350
    Epoch  36 / iter   1, loss = 0.0781
    Epoch  36 / iter   2, loss = 0.1458
    Epoch  36 / iter   3, loss = 0.1288
    Epoch  36 / iter   4, loss = 0.0421
    Epoch  37 / iter   0, loss = 0.1083
    Epoch  37 / iter   1, loss = 0.0972
    Epoch  37 / iter   2, loss = 0.1513
    Epoch  37 / iter   3, loss = 0.1236
    Epoch  37 / iter   4, loss = 0.0366
    Epoch  38 / iter   0, loss = 0.1204
    Epoch  38 / iter   1, loss = 0.1341
    Epoch  38 / iter   2, loss = 0.1109
    Epoch  38 / iter   3, loss = 0.0905
    Epoch  38 / iter   4, loss = 0.3906
    Epoch  39 / iter   0, loss = 0.0923
    Epoch  39 / iter   1, loss = 0.1094
    Epoch  39 / iter   2, loss = 0.1295
    Epoch  39 / iter   3, loss = 0.1239
    Epoch  39 / iter   4, loss = 0.0684
    Epoch  40 / iter   0, loss = 0.1188
    Epoch  40 / iter   1, loss = 0.0984
    Epoch  40 / iter   2, loss = 0.1067
    Epoch  40 / iter   3, loss = 0.1057
    Epoch  40 / iter   4, loss = 0.4602
    Epoch  41 / iter   0, loss = 0.1478
    Epoch  41 / iter   1, loss = 0.0980
    Epoch  41 / iter   2, loss = 0.0921
    Epoch  41 / iter   3, loss = 0.1020
    Epoch  41 / iter   4, loss = 0.0430
    Epoch  42 / iter   0, loss = 0.0991
    Epoch  42 / iter   1, loss = 0.0994
    Epoch  42 / iter   2, loss = 0.1270
    Epoch  42 / iter   3, loss = 0.0988
    Epoch  42 / iter   4, loss = 0.1176
    Epoch  43 / iter   0, loss = 0.1286
    Epoch  43 / iter   1, loss = 0.1013
    Epoch  43 / iter   2, loss = 0.1066
    Epoch  43 / iter   3, loss = 0.0779
    Epoch  43 / iter   4, loss = 0.1481
    Epoch  44 / iter   0, loss = 0.0840
    Epoch  44 / iter   1, loss = 0.0858
    Epoch  44 / iter   2, loss = 0.1388
    Epoch  44 / iter   3, loss = 0.1000
    Epoch  44 / iter   4, loss = 0.0313
    Epoch  45 / iter   0, loss = 0.0896
    Epoch  45 / iter   1, loss = 0.1173
    Epoch  45 / iter   2, loss = 0.0916
    Epoch  45 / iter   3, loss = 0.1043
    Epoch  45 / iter   4, loss = 0.0074
    Epoch  46 / iter   0, loss = 0.1008
    Epoch  46 / iter   1, loss = 0.0915
    Epoch  46 / iter   2, loss = 0.0877
    Epoch  46 / iter   3, loss = 0.1139
    Epoch  46 / iter   4, loss = 0.0292
    Epoch  47 / iter   0, loss = 0.0679
    Epoch  47 / iter   1, loss = 0.0987
    Epoch  47 / iter   2, loss = 0.0929
    Epoch  47 / iter   3, loss = 0.1098
    Epoch  47 / iter   4, loss = 0.4838
    Epoch  48 / iter   0, loss = 0.0693
    Epoch  48 / iter   1, loss = 0.1095
    Epoch  48 / iter   2, loss = 0.1128
    Epoch  48 / iter   3, loss = 0.0890
    Epoch  48 / iter   4, loss = 0.1008
    Epoch  49 / iter   0, loss = 0.0724
    Epoch  49 / iter   1, loss = 0.0804
    Epoch  49 / iter   2, loss = 0.0919
    Epoch  49 / iter   3, loss = 0.1233
    Epoch  49 / iter   4, loss = 0.1849



![png](output_88_1.png)


è§‚å¯Ÿä¸Šè¿°Lossçš„å˜åŒ–ï¼Œéšæœºæ¢¯åº¦ä¸‹é™åŠ å¿«äº†è®­ç»ƒè¿‡ç¨‹ï¼Œä½†ç”±äºæ¯æ¬¡ä»…åŸºäºå°‘é‡æ ·æœ¬æ›´æ–°å‚æ•°å’Œè®¡ç®—æŸå¤±ï¼Œæ‰€ä»¥æŸå¤±ä¸‹é™æ›²çº¿ä¼šå‡ºç°éœ‡è¡ã€‚

------
**è¯´æ˜ï¼š**

ç”±äºæˆ¿ä»·é¢„æµ‹çš„æ•°æ®é‡è¿‡å°‘ï¼Œæ‰€ä»¥éš¾ä»¥æ„Ÿå—åˆ°éšæœºæ¢¯åº¦ä¸‹é™å¸¦æ¥çš„æ€§èƒ½æå‡ã€‚

------

## æ€»ç»“

æœ¬èŠ‚ï¼Œæˆ‘ä»¬è¯¦ç»†è®²è§£äº†å¦‚ä½•ä½¿ç”¨Numpyå®ç°æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œæ„å»ºå¹¶è®­ç»ƒäº†ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹å®ç°æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ï¼Œå¯ä»¥æ€»ç»“å‡ºï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œå»ºæ¨¡æˆ¿ä»·é¢„æµ‹æœ‰ä¸‰ä¸ªè¦ç‚¹ï¼š

* æ„å»ºç½‘ç»œï¼Œåˆå§‹åŒ–å‚æ•°wå’Œbï¼Œå®šä¹‰é¢„æµ‹å’ŒæŸå¤±å‡½æ•°çš„è®¡ç®—æ–¹æ³•ã€‚
* éšæœºé€‰æ‹©åˆå§‹ç‚¹ï¼Œå»ºç«‹æ¢¯åº¦çš„è®¡ç®—æ–¹æ³•å’Œå‚æ•°æ›´æ–°æ–¹å¼ã€‚
* ä»æ€»çš„æ•°æ®é›†ä¸­æŠ½å–éƒ¨åˆ†æ•°æ®ä½œä¸ºä¸€ä¸ªmini_batchï¼Œè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°ï¼Œä¸æ–­è¿­ä»£ç›´åˆ°æŸå¤±å‡½æ•°å‡ ä¹ä¸å†ä¸‹é™ã€‚

### ä½œä¸š1-2

1. æ ·æœ¬å½’ä¸€åŒ–ï¼šé¢„æµ‹æ—¶çš„æ ·æœ¬æ•°æ®åŒæ ·ä¹Ÿéœ€è¦å½’ä¸€åŒ–ï¼Œä½†ä½¿ç”¨è®­ç»ƒæ ·æœ¬çš„å‡å€¼å’Œæå€¼è®¡ç®—ï¼Œè¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ

2. å½“éƒ¨åˆ†å‚æ•°çš„æ¢¯åº¦è®¡ç®—ä¸º0ï¼ˆæ¥è¿‘0ï¼‰æ—¶ï¼Œå¯èƒ½æ˜¯ä»€ä¹ˆæƒ…å†µï¼Ÿæ˜¯å¦æ„å‘³ç€å®Œæˆè®­ç»ƒï¼Ÿ

### ä½œä¸š 1-3

1. éšæœºæ¢¯åº¦ä¸‹é™çš„batchsizeè®¾ç½®æˆå¤šå°‘åˆé€‚ï¼Ÿè¿‡å°æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿè¿‡å¤§æœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿæç¤ºï¼šè¿‡å¤§ä»¥æ•´ä¸ªæ ·æœ¬é›†åˆä¸ºä¾‹ï¼Œè¿‡å°ä»¥å•ä¸ªæ ·æœ¬ä¸ºä¾‹æ¥æ€è€ƒã€‚
1. ä¸€æ¬¡è®­ç»ƒä½¿ç”¨çš„é…ç½®ï¼š5ä¸ªepochï¼Œ1000ä¸ªæ ·æœ¬ï¼Œbatchsize=20ï¼Œæœ€å†…å±‚å¾ªç¯æ‰§è¡Œå¤šå°‘è½®ï¼Ÿ


### ä½œä¸š1-4

#### åŸºæœ¬çŸ¥è¯†

**1. æ±‚å¯¼çš„é“¾å¼æ³•åˆ™**

é“¾å¼æ³•åˆ™æ˜¯å¾®ç§¯åˆ†ä¸­çš„æ±‚å¯¼æ³•åˆ™ï¼Œç”¨äºæ±‚ä¸€ä¸ªå¤åˆå‡½æ•°çš„å¯¼æ•°ï¼Œæ˜¯åœ¨å¾®ç§¯åˆ†çš„æ±‚å¯¼è¿ç®—ä¸­ä¸€ç§å¸¸ç”¨çš„æ–¹æ³•ã€‚å¤åˆå‡½æ•°çš„å¯¼æ•°å°†æ˜¯æ„æˆå¤åˆè¿™æœ‰é™ä¸ªå‡½æ•°åœ¨ç›¸åº”ç‚¹çš„å¯¼æ•°çš„ä¹˜ç§¯ï¼Œå°±åƒé”é“¾ä¸€æ ·ä¸€ç¯å¥—ä¸€ç¯ï¼Œæ•…ç§°é“¾å¼æ³•åˆ™ã€‚å¦‚ **å›¾9** æ‰€ç¤ºï¼Œå¦‚æœæ±‚æœ€ç»ˆè¾“å‡ºå¯¹å†…å±‚è¾“å…¥ï¼ˆç¬¬ä¸€å±‚ï¼‰çš„æ¢¯åº¦ï¼Œç­‰äºå¤–å±‚æ¢¯åº¦ï¼ˆç¬¬äºŒå±‚ï¼‰ä¹˜ä»¥æœ¬å±‚å‡½æ•°çš„æ¢¯åº¦ã€‚
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/a21b0bacbf554739965b47e8879679627dd015a8500e464c9dd08b58abf9d36f" width="300" hegiht="" ></center>
<center><br>å›¾9ï¼šæ±‚å¯¼çš„é“¾å¼æ³•åˆ™</br></center>
<br></br>

**2. è®¡ç®—å›¾çš„æ¦‚å¿µ**

ï¼ˆ1ï¼‰ä¸ºä½•æ˜¯åå‘è®¡ç®—æ¢¯åº¦ï¼Ÿå³æ¢¯åº¦æ˜¯ç”±ç½‘ç»œåç«¯å‘å‰ç«¯è®¡ç®—ã€‚å½“å‰å±‚çš„æ¢¯åº¦è¦ä¾æ®å¤„äºç½‘ç»œä¸­åä¸€å±‚çš„æ¢¯åº¦æ¥è®¡ç®—ï¼Œæ‰€ä»¥åªæœ‰å…ˆç®—åä¸€å±‚çš„æ¢¯åº¦æ‰èƒ½è®¡ç®—æœ¬å±‚çš„æ¢¯åº¦ã€‚     

ï¼ˆ2ï¼‰æ¡ˆä¾‹ï¼šè´­ä¹°è‹¹æœäº§ç”Ÿæ¶ˆè´¹çš„è®¡ç®—å›¾ã€‚å‡è®¾ä¸€å®¶å•†åº—9æŠ˜ä¿ƒé”€è‹¹æœï¼Œæ¯ä¸ªçš„å•ä»·100å…ƒã€‚è®¡ç®—ä¸€ä¸ªé¡¾å®¢æ€»æ¶ˆè´¹çš„ç»“æ„å¦‚ **å›¾10** æ‰€ç¤ºã€‚
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/3395f2e91ed743f1b30485ef47e70a55fa9f6b036ae94ddaad9046471c62e0c8" width="400" hegiht="40" ></center>
<center><br>å›¾10ï¼šè´­ä¹°è‹¹æœæ‰€äº§ç”Ÿçš„æ¶ˆè´¹è®¡ç®—å›¾</br></center>
<br></br>

*  å‰å‘è®¡ç®—è¿‡ç¨‹ï¼šä»¥é»‘è‰²ç®­å¤´è¡¨ç¤ºï¼Œé¡¾å®¢è´­ä¹°äº†2ä¸ªè‹¹æœï¼Œå†åŠ ä¸Šä¹æŠ˜çš„æŠ˜æ‰£ï¼Œä¸€å…±æ¶ˆè´¹100\*2\*0.9=180å…ƒã€‚
*  åå‘ä¼ æ’­è¿‡ç¨‹ï¼šä»¥çº¢è‰²ç®­å¤´è¡¨ç¤ºï¼Œæ ¹æ®é“¾å¼æ³•åˆ™ï¼Œæœ¬å±‚çš„æ¢¯åº¦è®¡ç®— * åä¸€å±‚ä¼ é€’è¿‡æ¥çš„æ¢¯åº¦ï¼Œæ‰€ä»¥éœ€ä»åå‘å‰è®¡ç®—ã€‚

æœ€åä¸€å±‚çš„è¾“å‡ºå¯¹è‡ªèº«çš„æ±‚å¯¼ä¸º1ã€‚å¯¼æ•°ç¬¬äºŒå±‚æ ¹æ® **å›¾11** æ‰€ç¤ºçš„ä¹˜æ³•æ±‚å¯¼çš„å…¬å¼ï¼Œåˆ†åˆ«ä¸º0.9\*1å’Œ200\*1ã€‚åŒæ ·çš„ï¼Œç¬¬ä¸‰å±‚ä¸º100 * 0.9=90ï¼Œ2 * 0.9=1.8ã€‚
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/719c0941e6934266b985cec5ead4806d876c3472435c4ba0a4888daf0ca356a4" width="120" hegiht="40" ></center>
<center><br>å›¾11ï¼šä¹˜æ³•æ±‚å¯¼çš„å…¬å¼</br></center>
<br></br>

#### ä½œä¸šé¢˜

1. æ ¹æ® **å›¾12** æ‰€ç¤ºçš„ä¹˜æ³•å’ŒåŠ æ³•çš„å¯¼æ•°å…¬å¼ï¼Œå®Œæˆ **å›¾13** è´­ä¹°è‹¹æœå’Œæ©˜å­çš„æ¢¯åº¦ä¼ æ’­çš„é¢˜ç›®ã€‚
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/472b4e351d314bc1bd37ea638cda6738ac668287c4364dc9bde4b7d5d9816be5" width="300" hegiht="40" ></center>
<center><br>å›¾12ï¼šä¹˜æ³•å’ŒåŠ æ³•çš„å¯¼æ•°å…¬å¼</br></center>
<br></br>

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/2d9ee05ce94b4e839e31df158d45f5f9e4bafd933e0442e4a07f40789628440e" width="500" hegiht="40" ></center>
<center><br>å›¾13ï¼šè´­ä¹°è‹¹æœå’Œæ©˜å­äº§ç”Ÿæ¶ˆè´¹çš„è®¡ç®—å›¾</br></center>
<br></br>  

2. æŒ‘æˆ˜é¢˜ï¼šç”¨ä»£ç å®ç°ä¸¤å±‚çš„ç¥ç»ç½‘ç»œçš„æ¢¯åº¦ä¼ æ’­ï¼Œä¸­é—´å±‚çš„å°ºå¯¸ä¸º13ã€æˆ¿ä»·é¢„æµ‹æ¡ˆä¾‹ã€‘ï¼ˆæ•™æ¡ˆå½“å‰çš„ç‰ˆæœ¬ä¸ºä¸€å±‚çš„ç¥ç»ç½‘ç»œï¼‰ï¼Œå¦‚ **å›¾14** æ‰€ç¤ºã€‚

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/9fdbc4bfe28d4fa39c982d8bb87a96f9447c6a7f71d64faa9d7105cd1565b776" width="300" hegiht="40" ></center>
<center><br>å›¾14ï¼šä¸¤å±‚çš„ç¥ç»ç½‘ç»œ</br></center>
<br></br>  
